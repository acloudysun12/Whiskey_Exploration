{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scipy import stats\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torchtext\n",
    "# from torchtext.data import get_tokenizer\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUSEFUL DOCUMENTATION\\nhttps://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\\nhttps://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\\ncharon.me posts\\nhttps://nicgian.github.io/text-generation-vae/ -- not successful version by someone using tf, used VAE?\\nhttps://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b\\nhttps://debuggercafe.com/sparse-autoencoders-using-kl-divergence-with-pytorch/\\nhttps://avandekleut.github.io/vae/\\nhttps://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/ -- pretty hands on, for an AE and for MNIST\\nhttps://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ -- combine into one corpus, then read?\\nhttps://stackoverflow.com/questions/45375488/how-to-filter-tokens-from-spacy-document -- used this to remove extra tokens\\nhttps://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "USEFUL DOCUMENTATION\n",
    "https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "charon.me posts\n",
    "https://nicgian.github.io/text-generation-vae/ -- not successful version by someone using tf, used VAE?\n",
    "https://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b\n",
    "https://debuggercafe.com/sparse-autoencoders-using-kl-divergence-with-pytorch/\n",
    "https://avandekleut.github.io/vae/\n",
    "https://analyticsindiamag.com/hands-on-guide-to-implement-deep-autoencoder-in-pytorch-for-image-reconstruction/ -- pretty hands on, for an AE and for MNIST\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ -- combine into one corpus, then read?\n",
    "https://stackoverflow.com/questions/45375488/how-to-filter-tokens-from-spacy-document -- used this to remove extra tokens\n",
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whiskey_type</th>\n",
       "      <th>whiskey_name</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rev_rating</th>\n",
       "      <th>rev_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>elbucko</td>\n",
       "      <td>Tasted December 16, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Tastes like whiskey, maybe some pear? Great on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>gmrocks</td>\n",
       "      <td>Tasted December 8, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>This one proved quite popular with group of fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>Mark-Willis</td>\n",
       "      <td>Tasted November 27, 2021</td>\n",
       "      <td>4.50</td>\n",
       "      <td>Surprise of the flight consisting of itself Gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>Dan-Cordial</td>\n",
       "      <td>Tasted November 13, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Floral notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>MoparRocker74</td>\n",
       "      <td>Tasted November 11, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Really good American single malt. Oaky and Cok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           whiskey_type                  whiskey_name  reviewer_name  \\\n",
       "0  american_single_malt  STRANAHAN'S COLORADO WHISKEY        elbucko   \n",
       "1  american_single_malt  STRANAHAN'S COLORADO WHISKEY        gmrocks   \n",
       "2  american_single_malt  STRANAHAN'S COLORADO WHISKEY    Mark-Willis   \n",
       "3  american_single_malt  STRANAHAN'S COLORADO WHISKEY    Dan-Cordial   \n",
       "4  american_single_malt  STRANAHAN'S COLORADO WHISKEY  MoparRocker74   \n",
       "\n",
       "                review_date  rev_rating  \\\n",
       "0  Tasted December 16, 2021        3.75   \n",
       "1   Tasted December 8, 2021        3.75   \n",
       "2  Tasted November 27, 2021        4.50   \n",
       "3  Tasted November 13, 2021        3.75   \n",
       "4  Tasted November 11, 2021        3.75   \n",
       "\n",
       "                                           rev_notes  \n",
       "0  Tastes like whiskey, maybe some pear? Great on...  \n",
       "1  This one proved quite popular with group of fr...  \n",
       "2  Surprise of the flight consisting of itself Gl...  \n",
       "3                                       Floral notes  \n",
       "4  Really good American single malt. Oaky and Cok...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ Read the file, check it out ################\n",
    "\n",
    "file_dir = './' \n",
    "\n",
    "df_ratings_all = pd.read_csv(file_dir + 'whisk_reviews_combined.csv')\n",
    "df_ratings_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL TRANSFORMS FOR DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ INITIAL NEW/TRANSFORMED COLS FOR DF ################\n",
    "\n",
    "rev_notes = df_ratings_all['rev_notes']\n",
    "rev_notes = [str(elem).encode(\"ascii\", \"ignore\").decode('utf-8') for elem in rev_notes] \n",
    "rev_notes = [re.sub('[\\n\\r\\t\\f]', ' ', elem) for elem in rev_notes] \n",
    "df_ratings_all['rev_notes'] = rev_notes # get the NEW rev notes\n",
    "\n",
    "len_review = [len(str(review)) for review in df_ratings_all['rev_notes']]\n",
    "df_ratings_all['rev_char_len'] = len_review\n",
    "\n",
    "df_ratings_all = df_ratings_all.loc[df_ratings_all['rev_char_len'] > 0, :].reset_index(drop = True) # aka get rid of empty reviews\n",
    "\n",
    "df_ratings_all['review_flg_pos'] = 1*(df_ratings_all['rev_rating'] >= 4) # DEFINE A POSITIVE REVIEW! READ SOME ABAOVE \n",
    "df_ratings_all['review_flg_neg'] = 1*(df_ratings_all['rev_rating'] < 3) # DEFINE A POSITIVE REVIEW! READ SOME ABAOVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    114483.000000\n",
       "mean        221.219858\n",
       "std         385.680698\n",
       "min           1.000000\n",
       "5%           11.000000\n",
       "10%          19.000000\n",
       "25%          41.000000\n",
       "35%          58.000000\n",
       "50%          93.000000\n",
       "65%         149.000000\n",
       "75%         216.000000\n",
       "90%         528.000000\n",
       "95%         945.900000\n",
       "97.5%      1403.000000\n",
       "max        7686.000000\n",
       "Name: rev_char_len, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_all['rev_char_len'].describe([0.05, 0.1, 0.25, 0.35, .5, 0.65, .75, 0.9, 0.95, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Light peat on the nose, light citrus. Medium body, lovely peat, spice, medium finish'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_all.loc[(df_ratings_all['rev_rating'] == 4),['rev_notes']].iloc[200][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET A SUBSET OF DATASET TO GENERATE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset dataset to get reviews from X to Y?\n",
    "\n",
    "df_ratings_subset_full = df_ratings_all.loc[(df_ratings_all['rev_char_len'] >= 200) & \n",
    "                                            (df_ratings_all['rev_char_len'] <= 500), :].reset_index(drop = True)\n",
    "df_ratings_subset_full = df_ratings_subset_full.loc[~np.isnan(df_ratings_subset_full['rev_rating'])].reset_index(drop = True)\n",
    "df_ratings_subset_full['rev_notes'] = df_ratings_subset_full['rev_notes'].str.lower() # convert to lower?\n",
    "df_ratings_subset_pos = df_ratings_subset_full.loc[df_ratings_subset_full['rev_rating'] >= 4, ].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_en = spacy.load('en_core_web_sm')\n",
    "nlp = tokenize_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10178\n"
     ]
    }
   ],
   "source": [
    "print(len(df_ratings_subset_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nose: barley, oak, lots of honey, tiny amount of cinnamon, love this nose  taste: serious cinnamon notes, oak spice, thick honey texture, balanced sweetness, slightly fruity   balance: 84/100 texture: 87/100 enjoyment: 85/100 overall: 85/100'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### PLAY AROUND WITH A FEW NUMBERS HERE ####\n",
    "# https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/#:~:text=In%20Spacy%2C%20the%20process%20of,matches%20the%20tokenizer%20exception%20rules.\n",
    "test_text = df_ratings_subset_pos['rev_notes'][10]\n",
    "test_doc = nlp(test_text)\n",
    "test_doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words as Counter\n",
    "all_words = Counter()\n",
    "for doc in nlp.pipe(df_ratings_subset_pos['rev_notes']):\n",
    "    words = [token.text for token in doc]\n",
    "    all_words.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16816\n",
      "8307\n"
     ]
    }
   ],
   "source": [
    "# find all words appearing 1 or fewer times! Remove from dimension\n",
    "small_words = {x: count for x, count in all_words.items() if count <= 1}\n",
    "print(len(all_words.items()))\n",
    "print(len(small_words.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_new_revs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.21132016181946"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_st = time.time()\n",
    "\n",
    "for doc in nlp.pipe(df_ratings_subset_pos['rev_notes']):\n",
    "#     print(doc.text)\n",
    "    indexes = []\n",
    "    for index, token in enumerate(doc):\n",
    "        if token.text in small_words.keys():\n",
    "            indexes.append(index)\n",
    "#     print(indexes)\n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "    np_array = np.delete(np_array, indexes, axis = 0)\n",
    "    doc2 = Doc(doc.vocab, words=[t.text for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    ls_new_revs.append(doc2.text)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "time_end - time_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_final_pos = df_ratings_subset_pos.copy()\n",
    "df_ratings_final_pos['new_rev_notes'] = ls_new_revs\n",
    "len_review = [len(str(review)) for review in df_ratings_final_pos['new_rev_notes']]\n",
    "df_ratings_final_pos['rev_char_len'] = len_review\n",
    "\n",
    "# after remove tokens, get back to >=200\n",
    "df_ratings_final_pos = df_ratings_final_pos.loc[(df_ratings_final_pos['rev_char_len'] >= 200), :].\\\n",
    "    reset_index(drop = True)\n",
    "\n",
    "# add in the final puncutation IF it is missing\n",
    "fin_rev_notes = [elem + '. ' if re.match('[?.!]', elem[len(elem)-2]) is None else elem \\\n",
    "                 for elem in df_ratings_final_pos['new_rev_notes']]\n",
    "\n",
    "# ADD EOS padding sa well, to indicate that we are on NEXT sentence\n",
    "# https://stackoverflow.com/questions/70346894/how-to-add-sos-token-to-keras-tokenizer\n",
    "fin_rev_notes = [elem + '<eos> ' for elem in fin_rev_notes]\n",
    "\n",
    "df_ratings_final_pos['new_rev_notes'] = fin_rev_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_text = df_ratings_final_pos['new_rev_notes'].str.cat(sep = ' ')\n",
    "\n",
    "# REMOVE double or more of ANY space type, where present. They mess up the array-forming b/c sequences do not match\n",
    "pattern= \"\\s+\"\n",
    "df_ratings_text = re.sub(pattern, \" \", df_ratings_text)\n",
    "\n",
    "df_ratings_text = df_ratings_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 659578\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "param_length_seq = 40 + 1 # 50 tokens is around 200 characters long!\n",
    "sequences = list()\n",
    "for i in range(param_length_seq, len(df_ratings_text)):\n",
    "    # select sequence of tokens\n",
    "    seq = df_ratings_text[i-param_length_seq:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the sequence of text doc\n",
    "# def save_doc(lines, filename):\n",
    "#     data = '\\n'.join(lines)\n",
    "#     file = open(filename, 'w')\n",
    "#     file.write(data)\n",
    "#     file.close()\n",
    "\n",
    "# save_doc(sequences, file_dir + 'whisk_reviews_sequence_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Keras API Tokenizer()\n",
    "tokenizer = Tokenizer(filters='\\t\\n') # we want to KEEP all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_array = np.random.randint(0, np.min([len(sequences), 500000]), size=300000, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Run the instantialized tokenzer() \n",
    "print(type(sequences))\n",
    "print(type(sequences[1000]))\n",
    "seq_small = np.array(sequences)[rand_array]\n",
    "seq_small = seq_small.tolist()\n",
    "\n",
    "# .fit_on_texts fits on a list of texts -- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "tokenizer.fit_on_texts(seq_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "# Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "sequence_ints = tokenizer.texts_to_sequences(seq_small)\n",
    "print(len(sequence_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8020\n"
     ]
    }
   ],
   "source": [
    "# Get vocab size, need + 1 because more easily can split out X and y training sets below\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[68262].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flavor forward . is it worth $ 50 yes , is it worth twice the special reserve , i m not sure but i am on the hunt for a bottle . <eos> non - store pick . sweet caramel and'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[29732]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSURE DIMENSION MATCHING -- IF ANYTHING PRINTED, DEBUG\n",
    "for i in range(len(sequence_ints)):\n",
    "    if len(sequence_ints[i]) != param_length_seq:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 41)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert ALL sequences into nparray format\n",
    "sequence_array = np.asarray(sequence_ints)\n",
    "sequence_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing input and output for our text data\n",
    "X = sequence_array[:,:-1] # this means take all number of docs (the rows), exclude the LAST word\n",
    "y = sequence_array[:,-1] # this means take all number of docts (the rows), take only the LAST word\n",
    "\n",
    "# Converts each y into an X dimensional vector matching vocab size!\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1] # get the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 64)            513280    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8020)              521300    \n",
      "=================================================================\n",
      "Total params: 1,071,764\n",
      "Trainable params: 1,071,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/48479915/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension -- maybe try lower embedding!!!\n",
    "# https://medium.com/deep-learning-with-keras/lstm-understanding-the-number-of-parameters-c4e087575756\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 64, input_length=seq_length))\n",
    "model.add(LSTM(64, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies what to run for loss and optimization\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy', tf.keras.metrics.CategoricalCrossentropy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2344/2344 - 106s - loss: 5.6451 - accuracy: 0.1098 - categorical_crossentropy: 5.6451\n",
      "Epoch 2/200\n",
      "2344/2344 - 106s - loss: 4.9423 - accuracy: 0.1812 - categorical_crossentropy: 4.9423\n",
      "Epoch 3/200\n",
      "2344/2344 - 107s - loss: 4.5770 - accuracy: 0.2127 - categorical_crossentropy: 4.5770\n",
      "Epoch 4/200\n",
      "2344/2344 - 107s - loss: 4.3412 - accuracy: 0.2308 - categorical_crossentropy: 4.3412\n",
      "Epoch 5/200\n",
      "2344/2344 - 106s - loss: 4.1678 - accuracy: 0.2444 - categorical_crossentropy: 4.1678\n",
      "Epoch 6/200\n",
      "2344/2344 - 106s - loss: 4.0305 - accuracy: 0.2546 - categorical_crossentropy: 4.0305\n",
      "Epoch 7/200\n",
      "2344/2344 - 107s - loss: 3.9157 - accuracy: 0.2643 - categorical_crossentropy: 3.9157\n",
      "Epoch 8/200\n",
      "2344/2344 - 107s - loss: 3.8163 - accuracy: 0.2728 - categorical_crossentropy: 3.8163\n",
      "Epoch 9/200\n",
      "2344/2344 - 106s - loss: 3.7275 - accuracy: 0.2809 - categorical_crossentropy: 3.7275\n",
      "Epoch 10/200\n",
      "2344/2344 - 106s - loss: 3.6459 - accuracy: 0.2892 - categorical_crossentropy: 3.6459\n",
      "Epoch 11/200\n",
      "2344/2344 - 106s - loss: 3.5739 - accuracy: 0.2975 - categorical_crossentropy: 3.5739\n",
      "Epoch 12/200\n",
      "2344/2344 - 107s - loss: 3.5059 - accuracy: 0.3050 - categorical_crossentropy: 3.5059\n",
      "Epoch 13/200\n",
      "2344/2344 - 106s - loss: 3.4435 - accuracy: 0.3135 - categorical_crossentropy: 3.4435\n",
      "Epoch 14/200\n",
      "2344/2344 - 105s - loss: 3.3854 - accuracy: 0.3212 - categorical_crossentropy: 3.3854\n",
      "Epoch 15/200\n",
      "2344/2344 - 106s - loss: 3.3312 - accuracy: 0.3282 - categorical_crossentropy: 3.3312\n",
      "Epoch 16/200\n",
      "2344/2344 - 106s - loss: 3.2807 - accuracy: 0.3358 - categorical_crossentropy: 3.2807\n",
      "Epoch 17/200\n",
      "2344/2344 - 123s - loss: 3.2338 - accuracy: 0.3427 - categorical_crossentropy: 3.2338\n",
      "Epoch 18/200\n",
      "2344/2344 - 156s - loss: 3.1880 - accuracy: 0.3498 - categorical_crossentropy: 3.1880\n",
      "Epoch 19/200\n",
      "2344/2344 - 154s - loss: 3.1456 - accuracy: 0.3562 - categorical_crossentropy: 3.1456\n",
      "Epoch 20/200\n",
      "2344/2344 - 155s - loss: 3.1048 - accuracy: 0.3618 - categorical_crossentropy: 3.1048\n",
      "Epoch 21/200\n",
      "2344/2344 - 156s - loss: 3.0663 - accuracy: 0.3678 - categorical_crossentropy: 3.0663\n",
      "Epoch 22/200\n",
      "2344/2344 - 155s - loss: 3.0291 - accuracy: 0.3735 - categorical_crossentropy: 3.0291\n",
      "Epoch 23/200\n",
      "2344/2344 - 156s - loss: 2.9952 - accuracy: 0.3797 - categorical_crossentropy: 2.9952\n",
      "Epoch 24/200\n",
      "2344/2344 - 155s - loss: 2.9613 - accuracy: 0.3848 - categorical_crossentropy: 2.9613\n",
      "Epoch 25/200\n",
      "2344/2344 - 157s - loss: 2.9286 - accuracy: 0.3904 - categorical_crossentropy: 2.9286\n",
      "Epoch 26/200\n",
      "2344/2344 - 155s - loss: 2.8989 - accuracy: 0.3952 - categorical_crossentropy: 2.8989\n",
      "Epoch 27/200\n",
      "2344/2344 - 158s - loss: 2.8707 - accuracy: 0.4000 - categorical_crossentropy: 2.8707\n",
      "Epoch 28/200\n",
      "2344/2344 - 155s - loss: 2.8405 - accuracy: 0.4044 - categorical_crossentropy: 2.8405\n",
      "Epoch 29/200\n",
      "2344/2344 - 156s - loss: 2.8136 - accuracy: 0.4091 - categorical_crossentropy: 2.8136\n",
      "Epoch 30/200\n",
      "2344/2344 - 156s - loss: 2.7887 - accuracy: 0.4137 - categorical_crossentropy: 2.7887\n",
      "Epoch 31/200\n",
      "2344/2344 - 157s - loss: 2.7636 - accuracy: 0.4184 - categorical_crossentropy: 2.7636\n",
      "Epoch 32/200\n",
      "2344/2344 - 155s - loss: 2.7392 - accuracy: 0.4225 - categorical_crossentropy: 2.7392\n",
      "Epoch 33/200\n",
      "2344/2344 - 155s - loss: 2.7192 - accuracy: 0.4258 - categorical_crossentropy: 2.7192\n",
      "Epoch 34/200\n",
      "2344/2344 - 156s - loss: 2.6949 - accuracy: 0.4296 - categorical_crossentropy: 2.6949\n",
      "Epoch 35/200\n",
      "2344/2344 - 155s - loss: 2.6741 - accuracy: 0.4331 - categorical_crossentropy: 2.6741\n",
      "Epoch 36/200\n",
      "2344/2344 - 156s - loss: 2.6537 - accuracy: 0.4373 - categorical_crossentropy: 2.6537\n",
      "Epoch 37/200\n",
      "2344/2344 - 156s - loss: 2.6345 - accuracy: 0.4406 - categorical_crossentropy: 2.6345\n",
      "Epoch 38/200\n",
      "2344/2344 - 155s - loss: 2.6151 - accuracy: 0.4442 - categorical_crossentropy: 2.6151\n",
      "Epoch 39/200\n",
      "2344/2344 - 155s - loss: 2.5970 - accuracy: 0.4476 - categorical_crossentropy: 2.5970\n",
      "Epoch 40/200\n",
      "2344/2344 - 156s - loss: 2.5794 - accuracy: 0.4508 - categorical_crossentropy: 2.5794\n",
      "Epoch 41/200\n",
      "2344/2344 - 155s - loss: 2.5620 - accuracy: 0.4535 - categorical_crossentropy: 2.5620\n",
      "Epoch 42/200\n",
      "2344/2344 - 156s - loss: 2.5474 - accuracy: 0.4562 - categorical_crossentropy: 2.5474\n",
      "Epoch 43/200\n",
      "2344/2344 - 155s - loss: 2.5316 - accuracy: 0.4590 - categorical_crossentropy: 2.5316\n",
      "Epoch 44/200\n",
      "2344/2344 - 156s - loss: 2.5148 - accuracy: 0.4617 - categorical_crossentropy: 2.5148\n",
      "Epoch 45/200\n",
      "2344/2344 - 155s - loss: 2.4997 - accuracy: 0.4651 - categorical_crossentropy: 2.4997\n",
      "Epoch 46/200\n",
      "2344/2344 - 156s - loss: 2.4852 - accuracy: 0.4670 - categorical_crossentropy: 2.4852\n",
      "Epoch 47/200\n",
      "2344/2344 - 155s - loss: 2.4717 - accuracy: 0.4696 - categorical_crossentropy: 2.4717\n",
      "Epoch 48/200\n",
      "2344/2344 - 156s - loss: 2.4568 - accuracy: 0.4720 - categorical_crossentropy: 2.4568\n",
      "Epoch 49/200\n",
      "2344/2344 - 156s - loss: 2.4452 - accuracy: 0.4746 - categorical_crossentropy: 2.4452\n",
      "Epoch 50/200\n",
      "2344/2344 - 158s - loss: 2.4316 - accuracy: 0.4769 - categorical_crossentropy: 2.4316\n",
      "Epoch 51/200\n",
      "2344/2344 - 156s - loss: 2.4191 - accuracy: 0.4791 - categorical_crossentropy: 2.4191\n",
      "Epoch 52/200\n",
      "2344/2344 - 152s - loss: 2.4081 - accuracy: 0.4813 - categorical_crossentropy: 2.4081\n",
      "Epoch 53/200\n",
      "2344/2344 - 150s - loss: 2.3978 - accuracy: 0.4827 - categorical_crossentropy: 2.3978\n",
      "Epoch 54/200\n",
      "2344/2344 - 152s - loss: 2.3844 - accuracy: 0.4853 - categorical_crossentropy: 2.3844\n",
      "Epoch 55/200\n",
      "2344/2344 - 151s - loss: 2.3724 - accuracy: 0.4876 - categorical_crossentropy: 2.3724\n",
      "Epoch 56/200\n",
      "2344/2344 - 124s - loss: 2.3632 - accuracy: 0.4885 - categorical_crossentropy: 2.3632\n",
      "Epoch 57/200\n",
      "2344/2344 - 102s - loss: 2.3545 - accuracy: 0.4907 - categorical_crossentropy: 2.3545\n",
      "Epoch 58/200\n",
      "2344/2344 - 102s - loss: 2.3451 - accuracy: 0.4921 - categorical_crossentropy: 2.3451\n",
      "Epoch 59/200\n",
      "2344/2344 - 102s - loss: 2.3319 - accuracy: 0.4947 - categorical_crossentropy: 2.3319\n",
      "Epoch 60/200\n",
      "2344/2344 - 102s - loss: 2.3250 - accuracy: 0.4955 - categorical_crossentropy: 2.3250\n",
      "Epoch 61/200\n",
      "2344/2344 - 102s - loss: 2.3136 - accuracy: 0.4983 - categorical_crossentropy: 2.3136\n",
      "Epoch 62/200\n",
      "2344/2344 - 102s - loss: 2.3055 - accuracy: 0.4992 - categorical_crossentropy: 2.3055\n",
      "Epoch 63/200\n",
      "2344/2344 - 103s - loss: 2.2959 - accuracy: 0.5010 - categorical_crossentropy: 2.2959\n",
      "Epoch 64/200\n",
      "2344/2344 - 102s - loss: 2.2869 - accuracy: 0.5033 - categorical_crossentropy: 2.2869\n",
      "Epoch 65/200\n",
      "2344/2344 - 102s - loss: 2.2784 - accuracy: 0.5038 - categorical_crossentropy: 2.2784\n",
      "Epoch 66/200\n",
      "2344/2344 - 102s - loss: 2.2716 - accuracy: 0.5057 - categorical_crossentropy: 2.2716\n",
      "Epoch 67/200\n",
      "2344/2344 - 102s - loss: 2.2598 - accuracy: 0.5076 - categorical_crossentropy: 2.2598\n",
      "Epoch 68/200\n",
      "2344/2344 - 102s - loss: 2.2538 - accuracy: 0.5079 - categorical_crossentropy: 2.2538\n",
      "Epoch 69/200\n",
      "2344/2344 - 102s - loss: 2.2479 - accuracy: 0.5100 - categorical_crossentropy: 2.2479\n",
      "Epoch 70/200\n",
      "2344/2344 - 102s - loss: 2.2403 - accuracy: 0.5109 - categorical_crossentropy: 2.2403\n",
      "Epoch 71/200\n",
      "2344/2344 - 102s - loss: 2.2310 - accuracy: 0.5127 - categorical_crossentropy: 2.2310\n",
      "Epoch 72/200\n",
      "2344/2344 - 104s - loss: 2.2248 - accuracy: 0.5140 - categorical_crossentropy: 2.2248\n",
      "Epoch 73/200\n",
      "2344/2344 - 102s - loss: 2.2211 - accuracy: 0.5147 - categorical_crossentropy: 2.2211\n",
      "Epoch 74/200\n",
      "2344/2344 - 102s - loss: 2.2105 - accuracy: 0.5164 - categorical_crossentropy: 2.2105\n",
      "Epoch 75/200\n",
      "2344/2344 - 102s - loss: 2.2050 - accuracy: 0.5179 - categorical_crossentropy: 2.2050\n",
      "Epoch 76/200\n",
      "2344/2344 - 102s - loss: 2.1999 - accuracy: 0.5183 - categorical_crossentropy: 2.1999\n",
      "Epoch 77/200\n",
      "2344/2344 - 102s - loss: 2.1933 - accuracy: 0.5195 - categorical_crossentropy: 2.1933\n",
      "Epoch 78/200\n",
      "2344/2344 - 103s - loss: 2.1878 - accuracy: 0.5209 - categorical_crossentropy: 2.1878\n",
      "Epoch 79/200\n",
      "2344/2344 - 103s - loss: 2.1771 - accuracy: 0.5224 - categorical_crossentropy: 2.1771\n",
      "Epoch 80/200\n",
      "2344/2344 - 102s - loss: 2.1784 - accuracy: 0.5221 - categorical_crossentropy: 2.1784\n",
      "Epoch 81/200\n",
      "2344/2344 - 105s - loss: 2.1687 - accuracy: 0.5238 - categorical_crossentropy: 2.1687\n",
      "Epoch 82/200\n",
      "2344/2344 - 107s - loss: 2.1629 - accuracy: 0.5251 - categorical_crossentropy: 2.1629\n",
      "Epoch 83/200\n",
      "2344/2344 - 108s - loss: 2.1655 - accuracy: 0.5245 - categorical_crossentropy: 2.1655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "2344/2344 - 106s - loss: 2.1517 - accuracy: 0.5268 - categorical_crossentropy: 2.1517\n",
      "Epoch 85/200\n",
      "2344/2344 - 105s - loss: 2.1468 - accuracy: 0.5276 - categorical_crossentropy: 2.1468\n",
      "Epoch 86/200\n",
      "2344/2344 - 106s - loss: 2.1439 - accuracy: 0.5285 - categorical_crossentropy: 2.1439\n",
      "Epoch 87/200\n",
      "2344/2344 - 106s - loss: 2.1389 - accuracy: 0.5293 - categorical_crossentropy: 2.1389\n",
      "Epoch 88/200\n",
      "2344/2344 - 105s - loss: 2.1292 - accuracy: 0.5318 - categorical_crossentropy: 2.1292\n",
      "Epoch 89/200\n",
      "2344/2344 - 106s - loss: 2.1303 - accuracy: 0.5311 - categorical_crossentropy: 2.1303\n",
      "Epoch 90/200\n",
      "2344/2344 - 106s - loss: 2.1219 - accuracy: 0.5328 - categorical_crossentropy: 2.1219\n",
      "Epoch 91/200\n",
      "2344/2344 - 105s - loss: 2.1195 - accuracy: 0.5332 - categorical_crossentropy: 2.1195\n",
      "Epoch 92/200\n",
      "2344/2344 - 106s - loss: 2.1126 - accuracy: 0.5352 - categorical_crossentropy: 2.1126\n",
      "Epoch 93/200\n",
      "2344/2344 - 106s - loss: 2.1101 - accuracy: 0.5347 - categorical_crossentropy: 2.1101\n",
      "Epoch 94/200\n",
      "2344/2344 - 105s - loss: 2.1015 - accuracy: 0.5369 - categorical_crossentropy: 2.1015\n",
      "Epoch 95/200\n",
      "2344/2344 - 106s - loss: 2.0980 - accuracy: 0.5377 - categorical_crossentropy: 2.0980\n",
      "Epoch 96/200\n",
      "2344/2344 - 106s - loss: 2.0934 - accuracy: 0.5379 - categorical_crossentropy: 2.0934\n",
      "Epoch 97/200\n",
      "2344/2344 - 106s - loss: 2.0925 - accuracy: 0.5383 - categorical_crossentropy: 2.0925\n",
      "Epoch 98/200\n",
      "2344/2344 - 106s - loss: 2.0920 - accuracy: 0.5378 - categorical_crossentropy: 2.0920\n",
      "Epoch 99/200\n",
      "2344/2344 - 106s - loss: 2.0823 - accuracy: 0.5403 - categorical_crossentropy: 2.0823\n",
      "Epoch 100/200\n",
      "2344/2344 - 106s - loss: 2.0815 - accuracy: 0.5405 - categorical_crossentropy: 2.0815\n",
      "Epoch 101/200\n",
      "2344/2344 - 106s - loss: 2.0847 - accuracy: 0.5390 - categorical_crossentropy: 2.0847\n",
      "Epoch 102/200\n",
      "2344/2344 - 106s - loss: 2.0717 - accuracy: 0.5413 - categorical_crossentropy: 2.0717\n",
      "Epoch 103/200\n",
      "2344/2344 - 106s - loss: 2.0667 - accuracy: 0.5430 - categorical_crossentropy: 2.0667\n",
      "Epoch 104/200\n",
      "2344/2344 - 106s - loss: 2.0636 - accuracy: 0.5426 - categorical_crossentropy: 2.0636\n",
      "Epoch 105/200\n",
      "2344/2344 - 105s - loss: 2.0602 - accuracy: 0.5443 - categorical_crossentropy: 2.0602\n",
      "Epoch 106/200\n",
      "2344/2344 - 106s - loss: 2.0557 - accuracy: 0.5442 - categorical_crossentropy: 2.0557\n",
      "Epoch 107/200\n",
      "2344/2344 - 106s - loss: 2.0535 - accuracy: 0.5452 - categorical_crossentropy: 2.0535\n",
      "Epoch 108/200\n",
      "2344/2344 - 106s - loss: 2.0520 - accuracy: 0.5454 - categorical_crossentropy: 2.0520\n",
      "Epoch 109/200\n",
      "2344/2344 - 106s - loss: 2.0454 - accuracy: 0.5463 - categorical_crossentropy: 2.0454\n",
      "Epoch 110/200\n",
      "2344/2344 - 106s - loss: 2.0476 - accuracy: 0.5460 - categorical_crossentropy: 2.0476\n",
      "Epoch 111/200\n",
      "2344/2344 - 106s - loss: 2.0445 - accuracy: 0.5465 - categorical_crossentropy: 2.0445\n",
      "Epoch 112/200\n",
      "2344/2344 - 105s - loss: 2.0368 - accuracy: 0.5473 - categorical_crossentropy: 2.0368\n",
      "Epoch 113/200\n",
      "2344/2344 - 106s - loss: 2.0355 - accuracy: 0.5481 - categorical_crossentropy: 2.0355\n",
      "Epoch 114/200\n",
      "2344/2344 - 105s - loss: 2.0344 - accuracy: 0.5479 - categorical_crossentropy: 2.0344\n",
      "Epoch 115/200\n",
      "2344/2344 - 107s - loss: 2.0297 - accuracy: 0.5496 - categorical_crossentropy: 2.0297\n",
      "Epoch 116/200\n",
      "2344/2344 - 105s - loss: 2.0231 - accuracy: 0.5498 - categorical_crossentropy: 2.0231\n",
      "Epoch 117/200\n",
      "2344/2344 - 107s - loss: 2.0235 - accuracy: 0.5497 - categorical_crossentropy: 2.0235\n",
      "Epoch 118/200\n",
      "2344/2344 - 106s - loss: 2.0201 - accuracy: 0.5513 - categorical_crossentropy: 2.0201\n",
      "Epoch 119/200\n",
      "2344/2344 - 106s - loss: 2.0151 - accuracy: 0.5517 - categorical_crossentropy: 2.0151\n",
      "Epoch 120/200\n",
      "2344/2344 - 105s - loss: 2.0096 - accuracy: 0.5528 - categorical_crossentropy: 2.0096\n",
      "Epoch 121/200\n",
      "2344/2344 - 106s - loss: 2.0089 - accuracy: 0.5536 - categorical_crossentropy: 2.0089\n",
      "Epoch 122/200\n",
      "2344/2344 - 106s - loss: 2.0063 - accuracy: 0.5542 - categorical_crossentropy: 2.0063\n",
      "Epoch 123/200\n",
      "2344/2344 - 106s - loss: 2.0050 - accuracy: 0.5538 - categorical_crossentropy: 2.0050\n",
      "Epoch 124/200\n",
      "2344/2344 - 106s - loss: 2.0024 - accuracy: 0.5545 - categorical_crossentropy: 2.0024\n",
      "Epoch 125/200\n",
      "2344/2344 - 105s - loss: 1.9964 - accuracy: 0.5549 - categorical_crossentropy: 1.9964\n",
      "Epoch 126/200\n",
      "2344/2344 - 106s - loss: 1.9965 - accuracy: 0.5556 - categorical_crossentropy: 1.9965\n",
      "Epoch 127/200\n",
      "2344/2344 - 105s - loss: 1.9969 - accuracy: 0.5543 - categorical_crossentropy: 1.9969\n",
      "Epoch 128/200\n",
      "2344/2344 - 105s - loss: 1.9920 - accuracy: 0.5559 - categorical_crossentropy: 1.9920\n",
      "Epoch 129/200\n",
      "2344/2344 - 107s - loss: 1.9891 - accuracy: 0.5566 - categorical_crossentropy: 1.9891\n",
      "Epoch 130/200\n",
      "2344/2344 - 105s - loss: 1.9906 - accuracy: 0.5565 - categorical_crossentropy: 1.9906\n",
      "Epoch 131/200\n",
      "2344/2344 - 105s - loss: 1.9873 - accuracy: 0.5568 - categorical_crossentropy: 1.9873\n",
      "Epoch 132/200\n",
      "2344/2344 - 106s - loss: 1.9829 - accuracy: 0.5582 - categorical_crossentropy: 1.9829\n",
      "Epoch 133/200\n",
      "2344/2344 - 105s - loss: 1.9791 - accuracy: 0.5584 - categorical_crossentropy: 1.9791\n",
      "Epoch 134/200\n",
      "2344/2344 - 105s - loss: 1.9766 - accuracy: 0.5585 - categorical_crossentropy: 1.9766\n",
      "Epoch 135/200\n",
      "2344/2344 - 105s - loss: 1.9744 - accuracy: 0.5588 - categorical_crossentropy: 1.9744\n",
      "Epoch 136/200\n",
      "2344/2344 - 105s - loss: 1.9738 - accuracy: 0.5591 - categorical_crossentropy: 1.9738\n",
      "Epoch 137/200\n",
      "2344/2344 - 105s - loss: 1.9751 - accuracy: 0.5586 - categorical_crossentropy: 1.9751\n",
      "Epoch 138/200\n",
      "2344/2344 - 105s - loss: 1.9672 - accuracy: 0.5609 - categorical_crossentropy: 1.9672\n",
      "Epoch 139/200\n",
      "2344/2344 - 105s - loss: 1.9677 - accuracy: 0.5599 - categorical_crossentropy: 1.9677\n",
      "Epoch 140/200\n",
      "2344/2344 - 105s - loss: 1.9644 - accuracy: 0.5607 - categorical_crossentropy: 1.9644\n",
      "Epoch 141/200\n",
      "2344/2344 - 106s - loss: 1.9625 - accuracy: 0.5610 - categorical_crossentropy: 1.9625\n",
      "Epoch 142/200\n",
      "2344/2344 - 105s - loss: 1.9597 - accuracy: 0.5619 - categorical_crossentropy: 1.9597\n",
      "Epoch 143/200\n",
      "2344/2344 - 105s - loss: 1.9618 - accuracy: 0.5606 - categorical_crossentropy: 1.9618\n",
      "Epoch 144/200\n",
      "2344/2344 - 105s - loss: 1.9548 - accuracy: 0.5623 - categorical_crossentropy: 1.9548\n",
      "Epoch 145/200\n",
      "2344/2344 - 105s - loss: 1.9509 - accuracy: 0.5632 - categorical_crossentropy: 1.9509\n",
      "Epoch 146/200\n",
      "2344/2344 - 105s - loss: 1.9518 - accuracy: 0.5631 - categorical_crossentropy: 1.9518\n",
      "Epoch 147/200\n",
      "2344/2344 - 106s - loss: 1.9507 - accuracy: 0.5625 - categorical_crossentropy: 1.9507\n",
      "Epoch 148/200\n",
      "2344/2344 - 105s - loss: 1.9505 - accuracy: 0.5636 - categorical_crossentropy: 1.9505\n",
      "Epoch 149/200\n",
      "2344/2344 - 106s - loss: 1.9476 - accuracy: 0.5638 - categorical_crossentropy: 1.9476\n",
      "Epoch 150/200\n",
      "2344/2344 - 117s - loss: 1.9422 - accuracy: 0.5642 - categorical_crossentropy: 1.9422\n",
      "Epoch 151/200\n",
      "2344/2344 - 108s - loss: 1.9429 - accuracy: 0.5646 - categorical_crossentropy: 1.9429\n",
      "Epoch 152/200\n",
      "2344/2344 - 106s - loss: 1.9353 - accuracy: 0.5663 - categorical_crossentropy: 1.9353\n",
      "Epoch 153/200\n",
      "2344/2344 - 105s - loss: 1.9360 - accuracy: 0.5664 - categorical_crossentropy: 1.9360\n",
      "Epoch 154/200\n",
      "2344/2344 - 105s - loss: 1.9400 - accuracy: 0.5644 - categorical_crossentropy: 1.9400\n",
      "Epoch 155/200\n",
      "2344/2344 - 106s - loss: 1.9330 - accuracy: 0.5663 - categorical_crossentropy: 1.9330\n",
      "Epoch 156/200\n",
      "2344/2344 - 106s - loss: 1.9325 - accuracy: 0.5665 - categorical_crossentropy: 1.9325\n",
      "Epoch 157/200\n",
      "2344/2344 - 106s - loss: 1.9329 - accuracy: 0.5657 - categorical_crossentropy: 1.9329\n",
      "Epoch 158/200\n",
      "2344/2344 - 106s - loss: 1.9290 - accuracy: 0.5664 - categorical_crossentropy: 1.9290\n",
      "Epoch 159/200\n",
      "2344/2344 - 105s - loss: 1.9319 - accuracy: 0.5660 - categorical_crossentropy: 1.9319\n",
      "Epoch 160/200\n",
      "2344/2344 - 105s - loss: 1.9243 - accuracy: 0.5681 - categorical_crossentropy: 1.9243\n",
      "Epoch 161/200\n",
      "2344/2344 - 105s - loss: 1.9230 - accuracy: 0.5677 - categorical_crossentropy: 1.9230\n",
      "Epoch 162/200\n",
      "2344/2344 - 105s - loss: 1.9279 - accuracy: 0.5675 - categorical_crossentropy: 1.9279\n",
      "Epoch 163/200\n",
      "2344/2344 - 106s - loss: 1.9238 - accuracy: 0.5678 - categorical_crossentropy: 1.9238\n",
      "Epoch 164/200\n",
      "2344/2344 - 105s - loss: 1.9576 - accuracy: 0.5602 - categorical_crossentropy: 1.9576\n",
      "Epoch 165/200\n",
      "2344/2344 - 106s - loss: 1.9100 - accuracy: 0.5712 - categorical_crossentropy: 1.9100\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344/2344 - 105s - loss: 1.9187 - accuracy: 0.5687 - categorical_crossentropy: 1.9187\n",
      "Epoch 167/200\n",
      "2344/2344 - 106s - loss: 1.9264 - accuracy: 0.5669 - categorical_crossentropy: 1.9264\n",
      "Epoch 168/200\n",
      "2344/2344 - 105s - loss: 1.9155 - accuracy: 0.5694 - categorical_crossentropy: 1.9155\n",
      "Epoch 169/200\n",
      "2344/2344 - 105s - loss: 1.9137 - accuracy: 0.5698 - categorical_crossentropy: 1.9137\n",
      "Epoch 170/200\n",
      "2344/2344 - 105s - loss: 1.9121 - accuracy: 0.5706 - categorical_crossentropy: 1.9121\n",
      "Epoch 171/200\n",
      "2344/2344 - 105s - loss: 1.9190 - accuracy: 0.5674 - categorical_crossentropy: 1.9190\n",
      "Epoch 172/200\n",
      "2344/2344 - 105s - loss: 1.9082 - accuracy: 0.5706 - categorical_crossentropy: 1.9082\n",
      "Epoch 173/200\n",
      "2344/2344 - 105s - loss: 1.9165 - accuracy: 0.5684 - categorical_crossentropy: 1.9165\n",
      "Epoch 174/200\n",
      "2344/2344 - 105s - loss: 1.9112 - accuracy: 0.5695 - categorical_crossentropy: 1.9112\n",
      "Epoch 175/200\n",
      "2344/2344 - 106s - loss: 1.9104 - accuracy: 0.5697 - categorical_crossentropy: 1.9104\n",
      "Epoch 176/200\n",
      "2344/2344 - 105s - loss: 1.9012 - accuracy: 0.5716 - categorical_crossentropy: 1.9012\n",
      "Epoch 177/200\n",
      "2344/2344 - 105s - loss: 1.8971 - accuracy: 0.5727 - categorical_crossentropy: 1.8971\n",
      "Epoch 178/200\n",
      "2344/2344 - 105s - loss: 1.9054 - accuracy: 0.5713 - categorical_crossentropy: 1.9054\n",
      "Epoch 179/200\n",
      "2344/2344 - 105s - loss: 1.9040 - accuracy: 0.5710 - categorical_crossentropy: 1.9040\n",
      "Epoch 180/200\n",
      "2344/2344 - 106s - loss: 1.8975 - accuracy: 0.5723 - categorical_crossentropy: 1.8975\n",
      "Epoch 181/200\n",
      "2344/2344 - 106s - loss: 1.9003 - accuracy: 0.5723 - categorical_crossentropy: 1.9003\n",
      "Epoch 182/200\n",
      "2344/2344 - 106s - loss: 1.8965 - accuracy: 0.5727 - categorical_crossentropy: 1.8965\n",
      "Epoch 183/200\n",
      "2344/2344 - 106s - loss: 1.8993 - accuracy: 0.5720 - categorical_crossentropy: 1.8993\n",
      "Epoch 184/200\n",
      "2344/2344 - 106s - loss: 1.8995 - accuracy: 0.5723 - categorical_crossentropy: 1.8995\n",
      "Epoch 185/200\n",
      "2344/2344 - 107s - loss: 1.8945 - accuracy: 0.5733 - categorical_crossentropy: 1.8945\n",
      "Epoch 186/200\n",
      "2344/2344 - 106s - loss: 1.8996 - accuracy: 0.5726 - categorical_crossentropy: 1.8996\n",
      "Epoch 187/200\n",
      "2344/2344 - 105s - loss: 1.9102 - accuracy: 0.5694 - categorical_crossentropy: 1.9102\n",
      "Epoch 188/200\n",
      "2344/2344 - 105s - loss: 1.9052 - accuracy: 0.5713 - categorical_crossentropy: 1.9052\n",
      "Epoch 189/200\n",
      "2344/2344 - 106s - loss: 1.8904 - accuracy: 0.5737 - categorical_crossentropy: 1.8904\n",
      "Epoch 190/200\n",
      "2344/2344 - 106s - loss: 1.8884 - accuracy: 0.5745 - categorical_crossentropy: 1.8884\n",
      "Epoch 191/200\n",
      "2344/2344 - 105s - loss: 1.8924 - accuracy: 0.5733 - categorical_crossentropy: 1.8924\n",
      "Epoch 192/200\n",
      "2344/2344 - 107s - loss: 1.8852 - accuracy: 0.5748 - categorical_crossentropy: 1.8852\n",
      "Epoch 193/200\n",
      "2344/2344 - 106s - loss: 1.8931 - accuracy: 0.5731 - categorical_crossentropy: 1.8931\n",
      "Epoch 194/200\n",
      "2344/2344 - 106s - loss: 1.8853 - accuracy: 0.5743 - categorical_crossentropy: 1.8853\n",
      "Epoch 195/200\n",
      "2344/2344 - 106s - loss: 1.8777 - accuracy: 0.5768 - categorical_crossentropy: 1.8777\n",
      "Epoch 196/200\n",
      "2344/2344 - 106s - loss: 1.8904 - accuracy: 0.5741 - categorical_crossentropy: 1.8904\n",
      "Epoch 197/200\n",
      "2344/2344 - 106s - loss: 1.8777 - accuracy: 0.5768 - categorical_crossentropy: 1.8777\n",
      "Epoch 198/200\n",
      "2344/2344 - 106s - loss: 1.8774 - accuracy: 0.5769 - categorical_crossentropy: 1.8774\n",
      "Epoch 199/200\n",
      "2344/2344 - 105s - loss: 1.8873 - accuracy: 0.5744 - categorical_crossentropy: 1.8873\n",
      "Epoch 200/200\n",
      "2344/2344 - 106s - loss: 1.8732 - accuracy: 0.5771 - categorical_crossentropy: 1.8732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda387e5e20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=200, shuffle=True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/#:~:text=In%20Spacy%2C%20the%20process%20of,matches%20the%20tokenizer%20exception%20rules.\n",
    "# https://spacy.io/usage/processing-pipelines\n",
    "# https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ -- word-level RNN in Keras!\n",
    "# https://towardsdatascience.com/text-generation-with-bi-lstm-in-pytorch-5fda6e7cc22c --> character-level text generation with Pytorch\n",
    "# https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html --> IS THIS WHAT I WANT? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('./model_pos_v7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "pickle.dump(tokenizer, open('./tokenizer_pos_v7.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict / Generate New Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('./model_pos_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod the tokenizer\n",
    "tokenizer = load(open('tokenizer_pos_v1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reminds me a lot of buff trace but more readily available . sweet vanilla / leathery nose . after adding a few drops of water , i can definitely see what they mean by smooth . vanilla , butterscotch , carmel on the palate with some spice . nice sweet ,\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "seed_text = sequences[120395]\n",
    "print(seed_text)\n",
    "print(len(seed_text.split(' '))) # this shows there are 51 elements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])\n",
    "\n",
    "len(encoded[0]) # HOW COME THIS IS 50, NOT 51?\n",
    "# encoded = encoded[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction as follows\n",
    "yhat = model.predict_classes(encoded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cinnamon\n"
     ]
    }
   ],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break\n",
    "print(out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". on the palate , the toasted coconut transitions to a sweeter honeysuckle . from there , the flavor smoothly slides into a deeper , richer , caramelized banana custard , with flairs of smoke and oak . floral , grassy , followed by honey and dark fruit aroma . the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"same mouth feel , the same mouth feel , the highland apparently and made it were out or or sticking 2 dilution this for the price . i do nt know what i have some other higher proof bourbon . i do n't love . nice , this one is\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = sequences[527121]\n",
    "print(seed_text)\n",
    "generate_seq(model, tokenizer, seq_length, seed_text = seed_text, n_words = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data, Build Vocab"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
