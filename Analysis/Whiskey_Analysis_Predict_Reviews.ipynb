{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scipy import stats\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import spacy\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>whiskey_type</th>\n",
       "      <th>whiskey_name</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rev_rating</th>\n",
       "      <th>rev_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>elbucko</td>\n",
       "      <td>Tasted December 16, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Tastes like whiskey, maybe some pear? Great on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>gmrocks</td>\n",
       "      <td>Tasted December 8, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>This one proved quite popular with group of fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>Mark-Willis</td>\n",
       "      <td>Tasted November 27, 2021</td>\n",
       "      <td>4.50</td>\n",
       "      <td>Surprise of the flight consisting of itself Gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>Dan-Cordial</td>\n",
       "      <td>Tasted November 13, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Floral notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>american_single_malt</td>\n",
       "      <td>STRANAHAN'S COLORADO WHISKEY</td>\n",
       "      <td>MoparRocker74</td>\n",
       "      <td>Tasted November 11, 2021</td>\n",
       "      <td>3.75</td>\n",
       "      <td>Really good American single malt. Oaky and Cok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           whiskey_type                  whiskey_name  reviewer_name  \\\n",
       "0  american_single_malt  STRANAHAN'S COLORADO WHISKEY        elbucko   \n",
       "1  american_single_malt  STRANAHAN'S COLORADO WHISKEY        gmrocks   \n",
       "2  american_single_malt  STRANAHAN'S COLORADO WHISKEY    Mark-Willis   \n",
       "3  american_single_malt  STRANAHAN'S COLORADO WHISKEY    Dan-Cordial   \n",
       "4  american_single_malt  STRANAHAN'S COLORADO WHISKEY  MoparRocker74   \n",
       "\n",
       "                review_date  rev_rating  \\\n",
       "0  Tasted December 16, 2021        3.75   \n",
       "1   Tasted December 8, 2021        3.75   \n",
       "2  Tasted November 27, 2021        4.50   \n",
       "3  Tasted November 13, 2021        3.75   \n",
       "4  Tasted November 11, 2021        3.75   \n",
       "\n",
       "                                           rev_notes  \n",
       "0  Tastes like whiskey, maybe some pear? Great on...  \n",
       "1  This one proved quite popular with group of fr...  \n",
       "2  Surprise of the flight consisting of itself Gl...  \n",
       "3                                       Floral notes  \n",
       "4  Really good American single malt. Oaky and Cok...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ Read the file, check it out ################\n",
    "\n",
    "file_dir = './' \n",
    "\n",
    "df_ratings_all = pd.read_csv(file_dir + 'whisk_reviews_combined.csv')\n",
    "df_ratings_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL TRANSFORMS FOR DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ INITIAL NEW/TRANSFORMED COLS FOR DF ################\n",
    "\n",
    "rev_notes = df_ratings_all['rev_notes']\n",
    "rev_notes = [str(elem).encode(\"ascii\", \"ignore\").decode('utf-8') for elem in rev_notes] \n",
    "rev_notes = [re.sub('[\\n\\r\\t\\f]', '. ', elem) for elem in rev_notes] # INSTEAD OF SPACE, PERIOD + SPACE\n",
    "df_ratings_all['rev_notes'] = rev_notes # get the NEW rev notes\n",
    "\n",
    "len_review = [len(str(review)) for review in df_ratings_all['rev_notes']]\n",
    "df_ratings_all['rev_char_len'] = len_review\n",
    "\n",
    "df_ratings_all = df_ratings_all.loc[df_ratings_all['rev_char_len'] > 0, :].reset_index(drop = True) # aka get rid of empty reviews\n",
    "\n",
    "# CREATE a LOW/HIGH flag\n",
    "df_ratings_all['review_flg'] = 1*(df_ratings_all['rev_rating'] >= 3.75) # DEFINE A POSITIVE REVIEW! READ SOME ABAOVE \n",
    "\n",
    "# OR CREATE a LOW/MED/HIGH flag. TRY TO FIGURE THIS OUT? \n",
    "# revivew_flg_vec = np.array([0]*len(df_ratings_all))\n",
    "# revivew_flg_vec[np.where(df_ratings_all['rev_rating'] >= 3)] = 1\n",
    "# revivew_flg_vec[np.where(df_ratings_all['rev_rating'] >= 4)] = 2\n",
    "# df_ratings_all['review_flg'] = revivew_flg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    114483.000000\n",
       "mean          0.625910\n",
       "std           0.483889\n",
       "min           0.000000\n",
       "5%            0.000000\n",
       "10%           0.000000\n",
       "25%           0.000000\n",
       "35%           0.000000\n",
       "50%           1.000000\n",
       "65%           1.000000\n",
       "75%           1.000000\n",
       "90%           1.000000\n",
       "95%           1.000000\n",
       "97.5%         1.000000\n",
       "max           1.000000\n",
       "Name: review_flg, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_all['review_flg'].describe([0.05, 0.1, 0.25, 0.35, .5, 0.65, .75, 0.9, 0.95, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smelling hints of cherry, vanilla, a backside of berry hints. Taste is a decent amount of butterscotch, light oak and a tad bit of spice. Still new to bourbon though so I could be all wrong'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_all.loc[(df_ratings_all['rev_rating'] == 3.25),['rev_notes']].iloc[282][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK UP INTO LONG / SHORT RATINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SHORT -- >=40, <200 (around 30th to 70th percentile)\n",
    "# LONG -- >=200 (cap at 2500 characters) (70th to 100th percentile)\n",
    "\n",
    "df_ratings_short_full = df_ratings_all.loc[(df_ratings_all['rev_char_len'] >= 50) & (df_ratings_all['rev_char_len'] < 500), :].\\\n",
    "    reset_index(drop = True)\n",
    "# THIS gets rid of reviews with no rating\n",
    "df_ratings_short_full = df_ratings_short_full.loc[~np.isnan(df_ratings_short_full['rev_rating'])].reset_index(drop = True)\n",
    "df_ratings_short = df_ratings_short_full.loc[:,['rev_notes', 'review_flg']]\n",
    "# df_ratings_short = df_ratings_short.rename(columns = {'rev_rating': 'review_flg'})\n",
    "\n",
    "df_ratings_long_full = df_ratings_all.loc[(df_ratings_all['rev_char_len'] >= 500), :].\\\n",
    "    reset_index(drop = True)\n",
    "rev_capped = [elem[0:2500] for elem in df_ratings_long_full['rev_notes']]\n",
    "df_ratings_long_full['rev_notes'] = rev_capped # THIS IS TO CAP LONG REVIEWS\n",
    "# THIS gets rid of reviews with no rating\n",
    "df_ratings_long_full = df_ratings_long_full.loc[~np.isnan(df_ratings_long_full['rev_rating'])].reset_index(drop = True)\n",
    "df_ratings_long = df_ratings_long_full.loc[:,['rev_notes', 'review_flg']]\n",
    "# df_ratings_long = df_ratings_long.rename(columns = {'rev_rating': 'review_flg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66473\n",
      "12150\n"
     ]
    }
   ],
   "source": [
    "print(len(df_ratings_short))\n",
    "print(len(df_ratings_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_notes</th>\n",
       "      <th>review_flg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tastes like whiskey, maybe some pear? Great on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This one proved quite popular with group of fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surprise of the flight consisting of itself Gl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Really good American single malt. Oaky and Cok...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cask strength. Strong ethanol nose, slight che...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           rev_notes  review_flg\n",
       "0  Tastes like whiskey, maybe some pear? Great on...           1\n",
       "1  This one proved quite popular with group of fr...           1\n",
       "2  Surprise of the flight consisting of itself Gl...           1\n",
       "3  Really good American single malt. Oaky and Cok...           1\n",
       "4  Cask strength. Strong ethanol nose, slight che...           0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_short.to_csv('df_ratings_short.csv', index = False)\n",
    "df_ratings_long.to_csv('df_ratings_long.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab for Short Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize = 'spacy', \n",
    "    tokenizer_language = 'en_core_web_sm'\n",
    "    )\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype = torch.long)\n",
    "# HOW TO CONVERT TO NUMERICAL?\n",
    "# LABEL = torchtext.legacy.data.LabelField(dtype = torch.long, use_vocab = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('rev_notes', TEXT), ('review_flg', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path = 'df_ratings_short.csv', format = 'csv', \n",
    "    skip_header = True, fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = dataset.split(split_ratio = [0.60, 0.20, 0.20], \n",
    "                                        random_state = random.seed(72033))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nose', ':', 'cherry', ',', 'vanilla', ',', 'oak', '.', 'Proof', 'coming', 'through', '.', 'Palate', ':', 'classic', 'BT', 'cherry', 'palate', 'but', 'with', 'a', 'kick', '.', 'More', 'oak', ',', 'leather', ',', 'and', 'caramels', '.', 'Finish', ':', 'very', 'oily', 'and', 'long', ',', 'almost', 'creme', 'brle', '-', 'like', 'at', 'the', 'very', 'end', '.', '.', 'Delicious', '.', 'Enough', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[2])['rev_notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30801\n",
      "Number of classes: 2\n",
      "[('.', 124326), (',', 77999), ('and', 42868), ('the', 40276), ('a', 37785), ('of', 25616), ('with', 18220), ('is', 16952), ('I', 16719), ('but', 14423), ('to', 14208), ('on', 13305), ('it', 13141), (':', 10796), ('finish', 10700), ('this', 10030), ('for', 9298), ('in', 8856), ('nose', 8808), ('sweet', 8778), ('vanilla', 8681), ('not', 7086), ('-', 6498), ('that', 6237), ('spice', 6032), ('taste', 5973), ('oak', 5822), ('like', 5712), ('The', 5637), ('smooth', 5613), ('caramel', 5175), ('..', 5086), ('as', 5083), ('very', 5076), ('good', 5049), ('notes', 5040), ('!', 4782), ('Nose', 4749), ('some', 4730), ('at', 4653), ('more', 4541), ('Very', 4508), ('A', 4455), ('bourbon', 4405), ('little', 4394), ('bit', 4157), ('flavor', 4143), ('you', 4121), ('my', 4032), ('palate', 3813)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', ',', 'and', 'the', 'a', 'of', 'with', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[0:10]) # itos = integer-to-string # FIRST, SECOND are UNKNOWN AND PAD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "# Convert string to integer!\n",
    "print(TEXT.vocab.stoi['salty']) # stoi = string-to-integer\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n",
      "Counter({'1': 25636, '0': 14248})\n"
     ]
    }
   ],
   "source": [
    "# NOTE THAT TARGETS ARE FLIPPED IN DICTIONARY!\n",
    "print(LABEL.vocab.stoi)\n",
    "print(LABEL.vocab.freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=True, # FOR PACKED LSTM, but why not always?\n",
    "         sort_key=lambda x: len(x.rev_notes),\n",
    "         device='cpu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([29, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([10, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train') \n",
    "for batch in train_loader: # WHY NO SENTENCE LENGTH OF 8? \n",
    "    print(f'Text matrix size: {batch.rev_notes.size()}')\n",
    "    print(f'Target vector size: {batch.review_flg.size()}')\n",
    "    break\n",
    "\n",
    "print('\\nTest:') \n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.rev_notes.size()}')\n",
    "    print(f'Target vector size: {batch.review_flg.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This one uses the Normal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDEFINE THE SETTINGS AS NEEDED\n",
    "LEARNING_RATE = 0.0025\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = 'cpu'\n",
    "EMBEDDING_DIM = 16 # PERHAPS DO NOT NEED THAT BIG EMBEDDING, PREVENT OVERFIT. n-input x embedding ~ 5k * DIM >>> 500k\n",
    "HIDDEN_DIM = 32\n",
    "NUM_CLASSES = 2\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # THIS WAS FOR AN RNN\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim,\n",
    "                                 num_layers = n_layers,\n",
    "                                 dropout = dropout)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim) # these are RAW non-activated values, but maybe okay\n",
    "#         self.final_act = torch.nn.Sigmoid() # now it gives me probabilities. Also, NO PARAMS NEEDED --> will take \"self\"\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        ## FOR PACKED LSTM\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded) # NOTICE THIS EITHER packed OR embedded\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0) # --> squeeze the sentence length since always just 1?\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "#         output = self.final_act(outraw) # LETS NOT ACTIVATE FINAL LAYER\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(164809) # 20937, 293674, 457943\n",
    "model = RNN(input_dim = len(TEXT.vocab),\n",
    "            embedding_dim = EMBEDDING_DIM,\n",
    "            hidden_dim = HIDDEN_DIM,\n",
    "            output_dim = NUM_CLASSES, # could use 1 for binary classification, but this for generalization\n",
    "            n_layers = N_LAYERS, \n",
    "            dropout = DROPOUT\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "# model.train()\n",
    "# for batch_idx, batch_data in enumerate(train_loader):\n",
    "#     # FOR PACKED LSTM\n",
    "#     features = batch_data.rev_notes\n",
    "#     features\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/312 | Loss: 0.6859\n",
      "Epoch: 001/015 | Batch 050/312 | Loss: 0.6649\n",
      "Epoch: 001/015 | Batch 100/312 | Loss: 0.6032\n",
      "Epoch: 001/015 | Batch 150/312 | Loss: 0.6337\n",
      "Epoch: 001/015 | Batch 200/312 | Loss: 0.6751\n",
      "Epoch: 001/015 | Batch 250/312 | Loss: 0.5878\n",
      "Epoch: 001/015 | Batch 300/312 | Loss: 0.6022\n",
      "training accuracy: 70.40%\n",
      "valid accuracy: 69.66%\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 002/015 | Batch 000/312 | Loss: 0.5254\n",
      "Epoch: 002/015 | Batch 050/312 | Loss: 0.5554\n",
      "Epoch: 002/015 | Batch 100/312 | Loss: 0.5480\n",
      "Epoch: 002/015 | Batch 150/312 | Loss: 0.5816\n",
      "Epoch: 002/015 | Batch 200/312 | Loss: 0.4983\n",
      "Epoch: 002/015 | Batch 250/312 | Loss: 0.5694\n",
      "Epoch: 002/015 | Batch 300/312 | Loss: 0.5700\n",
      "training accuracy: 74.95%\n",
      "valid accuracy: 70.29%\n",
      "Time elapsed: 0.27 min\n",
      "Epoch: 003/015 | Batch 000/312 | Loss: 0.5300\n",
      "Epoch: 003/015 | Batch 050/312 | Loss: 0.4825\n",
      "Epoch: 003/015 | Batch 100/312 | Loss: 0.4668\n",
      "Epoch: 003/015 | Batch 150/312 | Loss: 0.5146\n",
      "Epoch: 003/015 | Batch 200/312 | Loss: 0.5630\n",
      "Epoch: 003/015 | Batch 250/312 | Loss: 0.4531\n",
      "Epoch: 003/015 | Batch 300/312 | Loss: 0.4813\n",
      "training accuracy: 78.13%\n",
      "valid accuracy: 71.60%\n",
      "Time elapsed: 0.40 min\n",
      "Epoch: 004/015 | Batch 000/312 | Loss: 0.3843\n",
      "Epoch: 004/015 | Batch 050/312 | Loss: 0.7080\n",
      "Epoch: 004/015 | Batch 100/312 | Loss: 0.4769\n",
      "Epoch: 004/015 | Batch 150/312 | Loss: 0.4548\n",
      "Epoch: 004/015 | Batch 200/312 | Loss: 0.4142\n",
      "Epoch: 004/015 | Batch 250/312 | Loss: 0.4709\n",
      "Epoch: 004/015 | Batch 300/312 | Loss: 0.4796\n",
      "training accuracy: 80.12%\n",
      "valid accuracy: 71.51%\n",
      "Time elapsed: 0.53 min\n",
      "Epoch: 005/015 | Batch 000/312 | Loss: 0.3875\n",
      "Epoch: 005/015 | Batch 050/312 | Loss: 0.4813\n",
      "Epoch: 005/015 | Batch 100/312 | Loss: 0.4139\n",
      "Epoch: 005/015 | Batch 150/312 | Loss: 0.4581\n",
      "Epoch: 005/015 | Batch 200/312 | Loss: 0.3583\n",
      "Epoch: 005/015 | Batch 250/312 | Loss: 0.4096\n",
      "Epoch: 005/015 | Batch 300/312 | Loss: 0.4204\n",
      "training accuracy: 82.10%\n",
      "valid accuracy: 72.47%\n",
      "Time elapsed: 0.66 min\n",
      "Epoch: 006/015 | Batch 000/312 | Loss: 0.3754\n",
      "Epoch: 006/015 | Batch 050/312 | Loss: 0.4916\n",
      "Epoch: 006/015 | Batch 100/312 | Loss: 0.3929\n",
      "Epoch: 006/015 | Batch 150/312 | Loss: 0.3775\n",
      "Epoch: 006/015 | Batch 200/312 | Loss: 0.4267\n",
      "Epoch: 006/015 | Batch 250/312 | Loss: 0.4441\n",
      "Epoch: 006/015 | Batch 300/312 | Loss: 0.4630\n",
      "training accuracy: 83.55%\n",
      "valid accuracy: 72.72%\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 007/015 | Batch 000/312 | Loss: 0.4162\n",
      "Epoch: 007/015 | Batch 050/312 | Loss: 0.3620\n",
      "Epoch: 007/015 | Batch 100/312 | Loss: 0.4274\n",
      "Epoch: 007/015 | Batch 150/312 | Loss: 0.4168\n",
      "Epoch: 007/015 | Batch 200/312 | Loss: 0.3715\n",
      "Epoch: 007/015 | Batch 250/312 | Loss: 0.4682\n",
      "Epoch: 007/015 | Batch 300/312 | Loss: 0.2847\n",
      "training accuracy: 83.86%\n",
      "valid accuracy: 73.60%\n",
      "Time elapsed: 0.92 min\n",
      "Epoch: 008/015 | Batch 000/312 | Loss: 0.4426\n",
      "Epoch: 008/015 | Batch 050/312 | Loss: 0.3042\n",
      "Epoch: 008/015 | Batch 100/312 | Loss: 0.3537\n",
      "Epoch: 008/015 | Batch 150/312 | Loss: 0.2449\n",
      "Epoch: 008/015 | Batch 200/312 | Loss: 0.2485\n",
      "Epoch: 008/015 | Batch 250/312 | Loss: 0.4269\n",
      "Epoch: 008/015 | Batch 300/312 | Loss: 0.4216\n",
      "training accuracy: 85.84%\n",
      "valid accuracy: 71.51%\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 009/015 | Batch 000/312 | Loss: 0.2479\n",
      "Epoch: 009/015 | Batch 050/312 | Loss: 0.2571\n",
      "Epoch: 009/015 | Batch 100/312 | Loss: 0.2132\n",
      "Epoch: 009/015 | Batch 150/312 | Loss: 0.3820\n",
      "Epoch: 009/015 | Batch 200/312 | Loss: 0.2933\n",
      "Epoch: 009/015 | Batch 250/312 | Loss: 0.3482\n",
      "Epoch: 009/015 | Batch 300/312 | Loss: 0.3211\n",
      "training accuracy: 86.92%\n",
      "valid accuracy: 72.42%\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 010/015 | Batch 000/312 | Loss: 0.3692\n",
      "Epoch: 010/015 | Batch 050/312 | Loss: 0.3868\n",
      "Epoch: 010/015 | Batch 100/312 | Loss: 0.3042\n",
      "Epoch: 010/015 | Batch 150/312 | Loss: 0.3441\n",
      "Epoch: 010/015 | Batch 200/312 | Loss: 0.2563\n",
      "Epoch: 010/015 | Batch 250/312 | Loss: 0.2675\n",
      "Epoch: 010/015 | Batch 300/312 | Loss: 0.2952\n",
      "training accuracy: 88.17%\n",
      "valid accuracy: 71.27%\n",
      "Time elapsed: 1.30 min\n",
      "Epoch: 011/015 | Batch 000/312 | Loss: 0.2800\n",
      "Epoch: 011/015 | Batch 050/312 | Loss: 0.1965\n",
      "Epoch: 011/015 | Batch 100/312 | Loss: 0.3966\n",
      "Epoch: 011/015 | Batch 150/312 | Loss: 0.2516\n",
      "Epoch: 011/015 | Batch 200/312 | Loss: 0.2762\n",
      "Epoch: 011/015 | Batch 250/312 | Loss: 0.2215\n",
      "Epoch: 011/015 | Batch 300/312 | Loss: 0.3576\n",
      "training accuracy: 89.05%\n",
      "valid accuracy: 71.10%\n",
      "Time elapsed: 1.43 min\n",
      "Epoch: 012/015 | Batch 000/312 | Loss: 0.2037\n",
      "Epoch: 012/015 | Batch 050/312 | Loss: 0.1913\n",
      "Epoch: 012/015 | Batch 100/312 | Loss: 0.1628\n",
      "Epoch: 012/015 | Batch 150/312 | Loss: 0.3437\n",
      "Epoch: 012/015 | Batch 200/312 | Loss: 0.6707\n",
      "Epoch: 012/015 | Batch 250/312 | Loss: 0.3027\n",
      "Epoch: 012/015 | Batch 300/312 | Loss: 0.2695\n",
      "training accuracy: 89.62%\n",
      "valid accuracy: 70.54%\n",
      "Time elapsed: 1.56 min\n",
      "Epoch: 013/015 | Batch 000/312 | Loss: 0.0907\n",
      "Epoch: 013/015 | Batch 050/312 | Loss: 0.1326\n",
      "Epoch: 013/015 | Batch 100/312 | Loss: 0.3323\n",
      "Epoch: 013/015 | Batch 150/312 | Loss: 0.3178\n",
      "Epoch: 013/015 | Batch 200/312 | Loss: 0.2907\n",
      "Epoch: 013/015 | Batch 250/312 | Loss: 0.2750\n",
      "Epoch: 013/015 | Batch 300/312 | Loss: 0.3595\n",
      "training accuracy: 90.32%\n",
      "valid accuracy: 69.85%\n",
      "Time elapsed: 1.68 min\n",
      "Epoch: 014/015 | Batch 000/312 | Loss: 0.1712\n",
      "Epoch: 014/015 | Batch 050/312 | Loss: 0.3437\n",
      "Epoch: 014/015 | Batch 100/312 | Loss: 0.1510\n",
      "Epoch: 014/015 | Batch 150/312 | Loss: 0.4375\n",
      "Epoch: 014/015 | Batch 200/312 | Loss: 0.2022\n",
      "Epoch: 014/015 | Batch 250/312 | Loss: 0.4134\n",
      "Epoch: 014/015 | Batch 300/312 | Loss: 0.2969\n",
      "training accuracy: 90.81%\n",
      "valid accuracy: 69.50%\n",
      "Time elapsed: 1.81 min\n",
      "Epoch: 015/015 | Batch 000/312 | Loss: 0.1385\n",
      "Epoch: 015/015 | Batch 050/312 | Loss: 0.2524\n",
      "Epoch: 015/015 | Batch 100/312 | Loss: 0.1678\n",
      "Epoch: 015/015 | Batch 150/312 | Loss: 0.2480\n",
      "Epoch: 015/015 | Batch 200/312 | Loss: 0.2339\n",
      "Epoch: 015/015 | Batch 250/312 | Loss: 0.1233\n",
      "Epoch: 015/015 | Batch 300/312 | Loss: 0.2748\n",
      "training accuracy: 91.50%\n",
      "valid accuracy: 69.47%\n",
      "Time elapsed: 1.94 min\n",
      "Total Training Time: 1.94 min\n",
      "Test accuracy: 69.82%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        features = batch_data.rev_notes.to(DEVICE)\n",
    "        labels = batch_data.review_flg.to(DEVICE)\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "#         logits = model(text) # FOR NORMAL LSTM\n",
    "        logits = model(features) # FOR PACKED LSTM\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "torch.save(model.state_dict(), 'model_short_reviews_2class.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_short = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = model_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n",
    "    return prediction[0][0].item()\n",
    "\n",
    "# print('Probability positive:')\n",
    "# predict_sentiment(model, \"This is a pretty good whiskey, not that sweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_pos = df_ratings_short_full[df_ratings_short_full['rev_rating'] >= 4].reset_index(drop = True)\n",
    "df_short_neg = df_ratings_short_full[df_ratings_short_full['rev_rating'] <= 2.5].reset_index(drop = True)\n",
    "df_short_med = df_ratings_short_full[(df_ratings_short_full['rev_rating'] > 2.5) & \\\n",
    "                                     (df_ratings_short_full['rev_rating'] <= 3.5)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive Reviews: \n",
      "Dumped on 3/28/19 Barrel # 372 Proof 125.6. . AMAZING! Butter and barrel on the nose with hints of brown sugar. On the palate it is incredibly viscous, like eating buttery corn. Notes of barrel and some spice leads to a strong finish that seems to keep going. One of the best pours Ive ever had!\n",
      "Positive Rating Prediction - 0.999\n",
      "Actual Rating - 4.5\n",
      "Favorite bourbon of mine... Hard to find and retails in my area (Atlanta)for $50. If you see it don't buy it... Call me first.\n",
      "Positive Rating Prediction - 0.9938\n",
      "Actual Rating - 5.0\n",
      "This is batch 007. Bottled Jan. 2015. Lots of smoky, briny, oaky, and even meaty aromas. That seaside campfire evoked by Laphroaig 10 is all there. This is like someone took a log off that fire and hit me in the face with it. I'm not complaining, mind you. There are a lot of good vanilla and coffee flavors underneath it all too.\n",
      "Positive Rating Prediction - 0.9987\n",
      "Actual Rating - 5.0\n",
      "\n",
      "Negative Reviews: \n",
      "Nose is mushy apples almost a Smokey smell . . Taste over smoked apples\n",
      "Positive Rating Prediction - 0.0423\n",
      "Actual Rating - 1.0\n",
      "The nose is very bland, a little hint of vanilla and tannin. Hint of vanilla gives way to an overwhelming taste of toffee and peanut butter. Not much mouthfeel at all. I'm just not really a fan.\n",
      "Positive Rating Prediction - 0.0163\n",
      "Actual Rating - 2.25\n",
      "Poison. Not to the Bell Biv DeVoe type. The kill the rat in your basement type.\n",
      "Positive Rating Prediction - 0.006\n",
      "Actual Rating - 1.0\n",
      "Who, drinkable, but definitively needs some water to melt down the sharpness.\n",
      "Positive Rating Prediction - 0.0886\n",
      "Actual Rating - 2.0\n",
      "I loved all bourbon every the not good ones until this turd came along. I could always find something about a bourbon that I like even if its weak and low proof. I dont know what it is I'm assuming their yeast strain but I absolutely positively did not like it. Even giving it time to open up made things worse. The extra kick in the nuts was it ruined my pallet making a 40$ pour of Jos. Magnus Cigar blend #23 also taste bad. ( had the cB before and since I know its great)\n",
      "Positive Rating Prediction - 0.4134\n",
      "Actual Rating - 2.0\n"
     ]
    }
   ],
   "source": [
    "print('\\nPositive Reviews: ')\n",
    "random.seed(20369)\n",
    "rand_sample = random.sample(list(range(len(df_short_pos))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_short_pos['rev_notes'][samp_idx]\n",
    "    samp_rating = df_short_pos['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    pred_pos = predict_sentiment(model_short, samp_review)\n",
    "    print('Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))\n",
    "\n",
    "print('\\nNegative Reviews: ')\n",
    "random.seed(36283)\n",
    "rand_sample = random.sample(list(range(len(df_short_neg))), 5)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_short_neg['rev_notes'][samp_idx]\n",
    "    samp_rating = df_short_neg['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    pred_pos = predict_sentiment(model_short, samp_review)\n",
    "    print('Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Middle Reviews: \n",
      "Drinking at work :). A little nutty but heavy in the liquor. Good to mix being bottle in bond\n",
      "Positive Rating Prediction - 0.1099\n",
      "Actual Rating - 3.0\n",
      "Interesting taste overall. I was expecting only a subtle rum taste in the bourbon but it tastes closer to a rum/bourbon hybrid. Its hard to say what else I can taste because of the overwhelming rum taste. I think it is very good but not my particular taste.\n",
      "Positive Rating Prediction - 0.0258\n",
      "Actual Rating - 2.75\n",
      "By Heaven Hill. An 80 proof, younger version of Evan Williams. Solid, inexpensive bourbon.\n",
      "Positive Rating Prediction - 0.0453\n",
      "Actual Rating - 3.0\n",
      "Very supple, sweet and smooth. Tasty but lacks bite and character.\n",
      "Positive Rating Prediction - 0.96\n",
      "Actual Rating - 3.0\n",
      "Simple, easy profile with not much to get excited about. . Chalet\n",
      "Positive Rating Prediction - 0.1658\n",
      "Actual Rating - 3.0\n"
     ]
    }
   ],
   "source": [
    "print('\\nMiddle Reviews: ')\n",
    "random.seed(34047)\n",
    "rand_sample = random.sample(list(range(len(df_short_med))), 5)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_short_med['rev_notes'][samp_idx]\n",
    "    samp_rating = df_short_med['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    pred_pos = predict_sentiment(model_short, samp_review)\n",
    "    print('Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive Review: Best mouthfeel Ive had on a bourbon. Felt and tasted buttery. Great barrel presence and heat followed by caramel and spices. Very long finish as well. Landed a great bottle!\n",
      "Model Positive Rating Prediction - 0.9946\n",
      "\n",
      "\n",
      "Negative Review: I tried to like it. Knowing taste buds can be different at times I kept at it. Finished the bottle. At times there was a musty funk taste and then there was the taste without the mustiness. Its not for me.\n",
      "Model Positive Rating Prediction - 0.0235\n",
      "\n",
      "\n",
      "Middle Review: Nose of Red Apple and nutmeg hint of lemon. Palate of apple rye spice and clove. Finish has some heat medium\n",
      "Model Positive Rating Prediction - 0.559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_review = \\\n",
    "    'Best mouthfeel Ive had on a bourbon. Felt and tasted buttery. Great barrel presence and heat followed by caramel and spices. Very long finish as well. Landed a great bottle!'\n",
    "print('\\nPositive Review: ' + pos_review)\n",
    "pred_pos = predict_sentiment(RNN_model, pos_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')\n",
    "\n",
    "neg_review = \\\n",
    "    'I tried to like it. Knowing taste buds can be different at times I kept at it. Finished the bottle. At times there was a musty funk taste and then there was the taste without the mustiness. Its not for me.'\n",
    "print('\\nNegative Review: ' + neg_review)\n",
    "pred_pos = predict_sentiment(RNN_model, neg_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')\n",
    "\n",
    "med_review = \\\n",
    "    'Nose of Red Apple and nutmeg hint of lemon. Palate of apple rye spice and clove. Finish has some heat medium'\n",
    "print('\\nMiddle Review: ' + med_review)\n",
    "pred_pos = predict_sentiment(RNN_model, med_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_short_miss = df_ratings_all.loc[(df_ratings_all['rev_char_len'] >= 40) & (df_ratings_all['rev_char_len'] < 200), :].\\\n",
    "    reset_index(drop = True)\n",
    "df_ratings_short_miss = df_ratings_short_miss.loc[np.isnan(df_ratings_short_miss['rev_rating'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews without Rating: \n",
      "10/22/16 Harpeth Liquors & Wine. $34.99 + tax\n",
      "Predicted Rating - 0.0072\n",
      "Sweet Maple and citrus nose, spicy wood palate, nice finish\n",
      "Predicted Rating - 0.3714\n",
      "I'm not sure what I taste with this one. It was awful from start to finish. I strongly encourage you to get a sample bottle to try before you waste your money.\n",
      "Predicted Rating - 0.012\n",
      "Heerlijk. Boozy, overrijpe banaan, sinaasappel likeur. Boterig.\n",
      "Predicted Rating - 0.0006\n",
      "Fall 2021 Pick by Rishi's International Beverage\n",
      "Predicted Rating - 0.0048\n",
      "GlenFiddich Distillery - Dufftown 9/30/2019\n",
      "Predicted Rating - 0.0362\n",
      "Finally starting to see this in NC, $12.40/750ml\n",
      "Predicted Rating - 0.0301\n",
      "Good Old Fashioned made with brown sugar simple syrup.\n",
      "Predicted Rating - 0.8194\n",
      "Gibbys & The Packie pick. Barrel F220881. Filled 11/2/15. Infinity 3/13/21\n",
      "Predicted Rating - 0.0025\n",
      "A strong paint thinner aroma persuaded me not to fully enjoy this particular spirit.\n",
      "Predicted Rating - 0.0191\n"
     ]
    }
   ],
   "source": [
    "print('\\nReviews without Rating: ')\n",
    "random.seed(97623)\n",
    "rand_sample = random.sample(list(range(len(df_ratings_short_miss))), 10)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_ratings_short_miss['rev_notes'][samp_idx]\n",
    "    print(samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predict_sentiment(model_short, samp_review), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive Review: Citrus and cinnamon notes. Very clean\n",
      "Model Positive Rating Prediction - 0.8602\n",
      "\n",
      "\n",
      "Positive Review: Heinous. Serious contender for worst \"bourbon\" Ive ever tasted. Although at one year Im not sure how they can legally call this bourbon.\n",
      "Model Positive Rating Prediction - 0.0231\n",
      "\n",
      "\n",
      "Positive Review: Smells syrupy and fruity. In the mouth it is very light and bright. Slightly warms the back of the mouth and throat. A sweet wheat flavor linger well after.\n",
      "Model Positive Rating Prediction - 0.5589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unk_review = \\\n",
    "    'Citrus and cinnamon notes. Very clean'\n",
    "print('\\nPositive Review: ' + unk_review)\n",
    "pred_pos = predict_sentiment(RNN_model, unk_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')\n",
    "\n",
    "unk_review = \\\n",
    "    'Heinous. Serious contender for worst \"bourbon\" Ive ever tasted. Although at one year Im not sure how they can legally call this bourbon.'\n",
    "print('\\nPositive Review: ' + unk_review)\n",
    "pred_pos = predict_sentiment(RNN_model, unk_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')\n",
    "\n",
    "unk_review = \\\n",
    "    'Smells syrupy and fruity. In the mouth it is very light and bright. Slightly warms the back of the mouth and throat. A sweet wheat flavor linger well after.'\n",
    "print('\\nPositive Review: ' + unk_review)\n",
    "pred_pos = predict_sentiment(RNN_model, unk_review)\n",
    "print('Model Positive Rating Prediction - ' + str(round(pred_pos, 4)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a Positive Rating - 0.5715\n"
     ]
    }
   ],
   "source": [
    "### POTENTIAL PROBLEMS WITH OVERFITTING AND BAD DATA\n",
    "\n",
    "sentence_1 = \"Easily a top contender for a bottle NOT to purchase!! This is literal trash\"\n",
    "pred_pos = predict_sentiment(RNN_model, sentence_1)\n",
    "print('Probability of a Positive Rating - ' + str(round(pred_pos, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a Positive Rating - 0.0034\n"
     ]
    }
   ],
   "source": [
    "sentence_2 = \"Easily a top contender for a bottle NOT to buy... Nose: moldy. Palate: dirt. Finish: sewer.\"\n",
    "pred_pos = predict_sentiment(RNN_model, sentence_2)\n",
    "print('Probability of a Positive Rating - ' + str(round(pred_pos, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a Positive Rating - 0.2381\n"
     ]
    }
   ],
   "source": [
    "sentence_3 = \"Easily a top contender for a bottle to dump out.. This is literal trash..\"\n",
    "pred_pos = predict_sentiment(RNN_model, sentence_3)\n",
    "print('Probability of a Positive Rating - ' + str(round(pred_pos, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a Positive Rating - 0.4864\n"
     ]
    }
   ],
   "source": [
    "sentence_3 = \"Easily a top contender for a bottle to dump out!! This is literal trash!!\"\n",
    "pred_pos = predict_sentiment(RNN_model, sentence_3)\n",
    "print('Probability of a Positive Rating - ' + str(round(pred_pos, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOES IT PREDICT LONG REVIEWS AT ALL?\n",
    "df_long_pos = df_ratings_long_full[df_ratings_long_full['rev_rating'] >= 4].reset_index(drop = True)\n",
    "df_long_neg = df_ratings_long_full[df_ratings_long_full['rev_rating'] <= 2.5].reset_index(drop = True)\n",
    "\n",
    "print('\\nPositive LONG Reviews: ')\n",
    "rand_sample = random.sample(list(range(len(df_long_pos))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_long_pos['rev_notes'][samp_idx]\n",
    "    samp_rating = df_long_pos['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predict_sentiment(model_short, samp_review), 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))\n",
    "\n",
    "print('\\nNegative LONG Reviews: ')\n",
    "rand_sample = random.sample(list(range(len(df_long_neg))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_long_neg['rev_notes'][samp_idx]\n",
    "    samp_rating = df_long_neg['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predict_sentiment(model_short, samp_review), 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab for Long Reviews\n",
    "\n",
    "We will try using packed LSTM here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.legacy.data.Field(\n",
    "    tokenize = 'spacy', \n",
    "    tokenizer_language = 'en_core_web_sm',\n",
    "    include_lengths=True # FOR PACKED LSTM\n",
    ")\n",
    "\n",
    "LABEL = torchtext.legacy.data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('rev_notes', TEXT), ('review_flg', LABEL)]\n",
    "\n",
    "dataset = torchtext.legacy.data.TabularDataset(\n",
    "    path = 'df_ratings_long.csv', format = 'csv', \n",
    "    skip_header = True, fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.split(split_ratio = [0.8, 0.2], \n",
    "                                        random_state = random.seed(72033))\n",
    "train_data, valid_data = train_data.split(split_ratio = [0.8, 0.2],\n",
    "                                            random_state = random.seed(20373))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very', 'young', '.', 'If', 'this', 'was', '8', 'years', 'old', 'toc10', 'years', 'it', 'would', 'be', 'superb', '.', 'As', 'it', 'stands', ',', 'it', \"'s\", 'just', 'cuvee', 'sweet', 'young', 'corn', 'whiskey', 'with', 'that', 'alcohol', 'burn', 'and', 'green', 'taste', '...', 'Disappointed', 'for', 'the', 'price', '.', 'Hope', 'future', 'revisions', 'of', 'this', 'are', 'older']\n"
     ]
    }
   ],
   "source": [
    "# PRINT OUT A SAMPLE REVIEW -- first 100 words\n",
    "print(vars(train_data.examples[379])['rev_notes'][0:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43678\n",
      "Number of classes: 2\n",
      "[('.', 153316), (',', 137008), ('the', 79145), ('and', 74628), ('a', 71470), ('of', 47626), ('I', 43060), ('is', 41821), ('to', 32587), ('it', 30320), ('with', 26846), ('this', 24287), ('but', 23538), (' ', 22492), ('in', 20303), ('that', 18768), ('on', 16758), ('-', 16421), (':', 16386), ('for', 16165), ('The', 16075), (\"'s\", 12214), ('not', 11751), ('as', 10932), ('nose', 10797)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
    "print(f'Number of classes: {len(LABEL.vocab)}')\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n"
     ]
    }
   ],
   "source": [
    "# Convert string to integer!\n",
    "print(TEXT.vocab.stoi['subtle']) # stoi = string-to-integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n",
      "Counter({'1': 12641, '0': 6945})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "review_flg\n",
       "0    10784\n",
       "1    19818\n",
       "dtype: int64"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE THAT THEY ARE FLIPPED IN DICTIONARY!\n",
    "print(LABEL.vocab.stoi) \n",
    "print(LABEL.vocab.freqs) # note also longer reviews tend to be positive, ~ 2 to 1\n",
    "df_ratings_long.groupby(['review_flg']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = \\\n",
    "    torchtext.legacy.data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=True, # NEW. necessary for packed_padded_sequence\n",
    "         sort_key=lambda x: len(x.rev_notes),\n",
    "         device='cpu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Text matrix size: torch.Size([415, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Valid:\n",
      "Text matrix size: torch.Size([42, 128])\n",
      "Target vector size: torch.Size([128])\n",
      "\n",
      "Test:\n",
      "Text matrix size: torch.Size([42, 128])\n",
      "Target vector size: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print('Train') \n",
    "for batch in train_loader: # WHY NO SENTENCE LENGTH OF 42 WORDS?!?! PACKED LSTM DOES NOT FIX IT \n",
    "    print(f'Text matrix size: {batch.rev_notes[0].size()}')\n",
    "    print(f'Target vector size: {batch.review_flg.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nValid:')\n",
    "for batch in valid_loader:\n",
    "    print(f'Text matrix size: {batch.rev_notes[0].size()}')\n",
    "    print(f'Target vector size: {batch.review_flg.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:') \n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.rev_notes[0].size()}')\n",
    "    print(f'Target vector size: {batch.review_flg.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This one uses the PACKED LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDEFINE THE SETTINGS\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 10 # TAKES MUCH LONGER TO TRAIN -- SO 10 epochs is okay. ALSO AFTER THE FACT, RESULTS NO CHANGE after 10 epochs\n",
    "DEVICE = 'cpu'\n",
    "EMBEDDING_DIM = 128 # PERHAPS DO NOT NEED THAT BIG EMBEDDING, PREVENT OVERFIT\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # THIS WAS FOR AN RNN\n",
    "        #self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim) # now it's 2, but these are RAW non-activated values\n",
    "        self.final_act = torch.nn.Sigmoid() # now it gives me probabilities. Also, NO PARAMS NEEDED --> will take \"self\"\n",
    "        \n",
    "\n",
    "    def forward(self, text, text_length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # ebedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        ## NEW\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "\n",
    "        hidden.squeeze_(0) # --> squeeze the sentence length since always just 1?\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        \n",
    "        outraw = self.fc(hidden)\n",
    "        \n",
    "        output = self.final_act(outraw)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(63853)\n",
    "model = RNN(input_dim=len(TEXT.vocab),\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            output_dim=NUM_CLASSES # could use 1 for binary classification, but this for generalization\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred, num_examples = 0, 0\n",
    "\n",
    "        for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "            # NEW\n",
    "            features, text_length = batch_data.rev_notes\n",
    "            targets = batch_data.review_flg.to(DEVICE)\n",
    "            \n",
    "            logits = model(features, text_length)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "            num_examples += targets.size(0)\n",
    "\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000/154 | Loss: 0.6889\n",
      "Epoch: 001/015 | Batch 050/154 | Loss: 0.6487\n",
      "Epoch: 001/015 | Batch 100/154 | Loss: 0.6171\n",
      "Epoch: 001/015 | Batch 150/154 | Loss: 0.5613\n",
      "training accuracy: 72.88%\n",
      "valid accuracy: 69.36%\n",
      "Time elapsed: 3.95 min\n",
      "Epoch: 002/015 | Batch 000/154 | Loss: 0.6308\n",
      "Epoch: 002/015 | Batch 050/154 | Loss: 0.6306\n",
      "Epoch: 002/015 | Batch 100/154 | Loss: 0.5527\n",
      "Epoch: 002/015 | Batch 150/154 | Loss: 0.5577\n",
      "training accuracy: 81.50%\n",
      "valid accuracy: 73.73%\n",
      "Time elapsed: 7.57 min\n",
      "Epoch: 003/015 | Batch 000/154 | Loss: 0.5354\n",
      "Epoch: 003/015 | Batch 050/154 | Loss: 0.4731\n",
      "Epoch: 003/015 | Batch 100/154 | Loss: 0.5003\n",
      "Epoch: 003/015 | Batch 150/154 | Loss: 0.4906\n",
      "training accuracy: 86.65%\n",
      "valid accuracy: 74.26%\n",
      "Time elapsed: 11.08 min\n",
      "Epoch: 004/015 | Batch 000/154 | Loss: 0.4100\n",
      "Epoch: 004/015 | Batch 050/154 | Loss: 0.4411\n",
      "Epoch: 004/015 | Batch 100/154 | Loss: 0.4021\n",
      "Epoch: 004/015 | Batch 150/154 | Loss: 0.4249\n",
      "training accuracy: 88.32%\n",
      "valid accuracy: 74.41%\n",
      "Time elapsed: 14.55 min\n",
      "Epoch: 005/015 | Batch 000/154 | Loss: 0.4620\n",
      "Epoch: 005/015 | Batch 050/154 | Loss: 0.4679\n",
      "Epoch: 005/015 | Batch 100/154 | Loss: 0.4304\n",
      "Epoch: 005/015 | Batch 150/154 | Loss: 0.4175\n",
      "training accuracy: 89.41%\n",
      "valid accuracy: 73.98%\n",
      "Time elapsed: 18.00 min\n",
      "Epoch: 006/015 | Batch 000/154 | Loss: 0.4537\n",
      "Epoch: 006/015 | Batch 050/154 | Loss: 0.4401\n",
      "Epoch: 006/015 | Batch 100/154 | Loss: 0.3789\n",
      "Epoch: 006/015 | Batch 150/154 | Loss: 0.4281\n",
      "training accuracy: 91.47%\n",
      "valid accuracy: 73.75%\n",
      "Time elapsed: 21.45 min\n",
      "Epoch: 007/015 | Batch 000/154 | Loss: 0.3877\n",
      "Epoch: 007/015 | Batch 050/154 | Loss: 0.4131\n",
      "Epoch: 007/015 | Batch 100/154 | Loss: 0.3530\n",
      "Epoch: 007/015 | Batch 150/154 | Loss: 0.4339\n",
      "training accuracy: 93.00%\n",
      "valid accuracy: 75.29%\n",
      "Time elapsed: 31.18 min\n",
      "Epoch: 008/015 | Batch 000/154 | Loss: 0.3617\n",
      "Epoch: 008/015 | Batch 050/154 | Loss: 0.3992\n",
      "Epoch: 008/015 | Batch 100/154 | Loss: 0.3697\n",
      "Epoch: 008/015 | Batch 150/154 | Loss: 0.3555\n",
      "training accuracy: 93.31%\n",
      "valid accuracy: 74.98%\n",
      "Time elapsed: 34.64 min\n",
      "Epoch: 009/015 | Batch 000/154 | Loss: 0.3600\n",
      "Epoch: 009/015 | Batch 050/154 | Loss: 0.3849\n",
      "Epoch: 009/015 | Batch 100/154 | Loss: 0.3520\n",
      "Epoch: 009/015 | Batch 150/154 | Loss: 0.4081\n",
      "training accuracy: 93.89%\n",
      "valid accuracy: 75.37%\n",
      "Time elapsed: 38.05 min\n",
      "Epoch: 010/015 | Batch 000/154 | Loss: 0.3734\n",
      "Epoch: 010/015 | Batch 050/154 | Loss: 0.3717\n",
      "Epoch: 010/015 | Batch 100/154 | Loss: 0.4033\n",
      "Epoch: 010/015 | Batch 150/154 | Loss: 0.4220\n",
      "training accuracy: 93.92%\n",
      "valid accuracy: 75.04%\n",
      "Time elapsed: 41.42 min\n",
      "Epoch: 011/015 | Batch 000/154 | Loss: 0.3918\n",
      "Epoch: 011/015 | Batch 050/154 | Loss: 0.4083\n",
      "Epoch: 011/015 | Batch 100/154 | Loss: 0.3814\n",
      "Epoch: 011/015 | Batch 150/154 | Loss: 0.3834\n",
      "training accuracy: 93.63%\n",
      "valid accuracy: 75.65%\n",
      "Time elapsed: 44.78 min\n",
      "Epoch: 012/015 | Batch 000/154 | Loss: 0.4072\n",
      "Epoch: 012/015 | Batch 050/154 | Loss: 0.3820\n",
      "Epoch: 012/015 | Batch 100/154 | Loss: 0.3597\n",
      "Epoch: 012/015 | Batch 150/154 | Loss: 0.3487\n",
      "training accuracy: 94.25%\n",
      "valid accuracy: 75.29%\n",
      "Time elapsed: 48.19 min\n",
      "Epoch: 013/015 | Batch 000/154 | Loss: 0.3453\n",
      "Epoch: 013/015 | Batch 050/154 | Loss: 0.3307\n",
      "Epoch: 013/015 | Batch 100/154 | Loss: 0.3871\n",
      "Epoch: 013/015 | Batch 150/154 | Loss: 0.3773\n",
      "training accuracy: 94.37%\n",
      "valid accuracy: 74.59%\n",
      "Time elapsed: 51.51 min\n",
      "Epoch: 014/015 | Batch 000/154 | Loss: 0.4061\n",
      "Epoch: 014/015 | Batch 050/154 | Loss: 0.3779\n",
      "Epoch: 014/015 | Batch 100/154 | Loss: 0.3894\n",
      "Epoch: 014/015 | Batch 150/154 | Loss: 0.3599\n",
      "training accuracy: 94.58%\n",
      "valid accuracy: 75.43%\n",
      "Time elapsed: 54.88 min\n",
      "Epoch: 015/015 | Batch 000/154 | Loss: 0.3788\n",
      "Epoch: 015/015 | Batch 050/154 | Loss: 0.3745\n",
      "Epoch: 015/015 | Batch 100/154 | Loss: 0.3280\n",
      "Epoch: 015/015 | Batch 150/154 | Loss: 0.3911\n",
      "training accuracy: 94.60%\n",
      "valid accuracy: 75.22%\n",
      "Time elapsed: 58.26 min\n",
      "Total Training Time: 58.26 min\n",
      "Test accuracy: 74.98%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "\n",
    "        # NEW\n",
    "        features, text_length = batch_data.rev_notes\n",
    "        labels = batch_data.review_flg.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(features, text_length)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training accuracy: '\n",
    "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
    "              f'\\nvalid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
    "        \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_long_revs = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def predict(model, sentence):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        length = [len(indexed)]\n",
    "        tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        length_tensor = torch.LongTensor(length)\n",
    "        predict_probas = torch.nn.functional.softmax(model(tensor, length_tensor), dim=1)\n",
    "        predicted_label_index = torch.argmax(predict_probas)\n",
    "        predicted_label_proba = torch.max(predict_probas)\n",
    "        return predicted_label_index.item(), predicted_label_proba.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_pos = df_ratings_long_full[df_ratings_long_full['rev_rating'] >= 4].reset_index(drop = True)\n",
    "df_long_neg = df_ratings_long_full[df_ratings_long_full['rev_rating'] <= 2.5].reset_index(drop = True)\n",
    "df_long_med = df_ratings_long_full[(df_ratings_long_full['rev_rating'] > 2.5) & \\\n",
    "                                   (df_ratings_long_full['rev_rating'] < 4)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peated holds up best to ice, and Id say wakes up with the cold. Color suffers, but thats about it. Everything else is as it should be, if anything more sweetness shows up than I previously noticed... alls well that ends well.'"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = df_ratings_long_full['rev_notes'][203]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.2689]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "length = [len(indexed)]\n",
    "tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "tensor = tensor.unsqueeze(1)\n",
    "length_tensor = torch.LongTensor(length)\n",
    "predict_probas = torch.nn.functional.softmax(model_long_revs(tensor, length_tensor), dim=1)\n",
    "predict_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1000):\n",
    "    samp_review = df_ratings_long_full['rev_notes'][idx]\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    ls_lbl_index.append(predicted_label_index)\n",
    "    ls_all_scores.append(predicted_label_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive Reviews: \n",
      "The nose is to die for. I'd want this in a candle in my house burning 24/7. Strong oak and burnt caramel on the nose. The palette is surprisingly lacking compared to the nose with the same oak and caramel with very strong cinnamon spice accompanying it through the finish. Super long and spicy finish leaves you warm and comfy.\n",
      "Predicted Rating - 0.731\n",
      "Actual Rating - 4.25\n",
      "Nose: slight nuttiness, coconut, vanilla, quite sweet aside from that HH peanut thing  Palate: vanilla, sweet oak, almost a tea flavor, spice in the background, quite rich for a $10 bottle of bourbon  Finish: spicy and medium, but a crispy almost cola taste at the start  My favorite bottom shelf bourbon.\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 4.0\n",
      "Nov 2019. Clear medium gold. Medium small tulip. Aromatic oatmeal, cereal, sweet spicy, milo beverage, sweet maltose, brine, buckwheat soba noodles in nose. Medium full body with sweet malt beverage, farmyard, oatmeal, multigrain bread and gentle sweet smoke in palate. Good balance, medium long finish. What a nice sea spray and gentle smoke dram.\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 4.5\n",
      "\n",
      "Negative Reviews: \n",
      "Octomores are expensive. I am a fan of Octomore. Most of them are worth the price of admission. This one is not. Octomore has marketed themselves as the peatiest of all whiskies, and the 8.3 is the peatiest of all Octomores. You would expect such a whisky to be unforgettable, but it is not.  Being a fan of peat is like being a fan of hot sauce. If you are so inclined, there are plenty of hot sauces claiming to be at insane levels of heat, and they will hurt your palate, to be sure, but they are forgettable curiosities after the initial cataclysm on contact. Octomore 8.3 offers much less depth than other Octomores, but, I agree, it is the peatiest. This shit tastes like if your lung cancer got cancer. Wheareas previous Octomores taste remarkably mature for their age, this one tastes like it is 5 years old, and theres very little to draw from it rather than a hateful style of metallic Terminator T-800 peat.  I normally score the travel retail exclusive Octomores (x.2) as the worst Octomores, because I have a bias against TREs. But when some TRE Octomores are tasted blind against 8.3, this is my least favorite Octomore. However, I must still emphasize that I am a fan of Octomore, and I would still clearly consume this whisky if the price was right, valuing a bottle at $100.  Score: 0 (forgettable) How much does a bottle cost: $200-250 How much do I think a bottle is worth: $100\n",
      "Predicted Rating - 0.7307\n",
      "Actual Rating - 2.0\n",
      "The bottle is awesome! It will serve as an infinite bottle when Im done! The whiskey is 70% corn, not a fan. Very harsh. For 86 proof its got a bite! Would highly NOT recommend! Not bad in cocktail cause it holds its on due to the high corn/rye mash bill! Cheers!\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 1.0\n",
      "Tastes like nothing. It's 40% so I didn't expect much but wow. Not a lot going on. Minimal nose. No initial taste upon a sip. At the exhale there's a hint of....scotch. Glad this was a gift and not a purchase. I wouldn't even recommend it for anyone new to scotch because this is a horrible expression of what should be the beginnings of a beautiful hobby.\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 2.25\n",
      "\n",
      "Middling Reviews: \n",
      "I had a real problem with my first glass of this. Even a few drops of water didnt help. Way too much heat. But second glass a month later and found it sweeter. Could find the bourbon barrel. Some vanilla. Maybe I had a bad day that first glass.\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 3.75\n",
      "Well, what can I say. Probably the cheapest bottle I've had the chance to taste. Smells like a typical bourbon, sweet and chocolately, with a hint of vanilla and honey. Nice, I am a bourbon drinker and can't get enough of that opening. Suprisingly tasteful for it's price. Little wry, though. Notes of ripe dark fruits, sweet apple and honey. Kinda sharp in the throat, aftertaste is a bit watery en slightly bitter. A pinch of chocolate and fresh flowers. I'd have to say: it's amazing for it's price, but overall it's a mediocre bourbon. That's +0.25 for the suprise :-)\n",
      "Predicted Rating - 0.7274\n",
      "Actual Rating - 3.25\n",
      "Very spicy, I was hoping for something smoother somewhat reminds me of the knob creek 100 proof, it's not bad but it's not great. Can definitely get better for the price tag. It does become better with a bit of water but still not a bottle I'd be searching and driving for miles. I was curious so I got it, regret ? No, as it still drinkable neat, the only way i drink whiskey is w a little drop or two of water and it works with this one.\n",
      "Predicted Rating - 0.7253\n",
      "Actual Rating - 2.75\n"
     ]
    }
   ],
   "source": [
    "print('\\nPositive Reviews: ')\n",
    "random.seed(20369)\n",
    "rand_sample = random.sample(list(range(len(df_long_pos))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_long_pos['rev_notes'][samp_idx]\n",
    "    samp_rating = df_long_pos['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predicted_label_proba, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))\n",
    "\n",
    "print('\\nNegative Reviews: ')\n",
    "random.seed(36283)\n",
    "rand_sample = random.sample(list(range(len(df_long_neg))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_long_neg['rev_notes'][samp_idx]\n",
    "    samp_rating = df_long_neg['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predicted_label_proba, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))\n",
    "\n",
    "print('\\nMiddling Reviews: ')\n",
    "rand_sample = random.sample(list(range(len(df_long_med))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_long_med['rev_notes'][samp_idx]\n",
    "    samp_rating = df_long_med['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predicted_label_proba, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positive SHORT Reviews: \n",
      "Smell of Granny Smith Apple and Caramel...taste of Apple and cinnamon...my favorite\n",
      "Predicted Rating - 0.6837\n",
      "Actual Rating - 5.0\n",
      "Smoky bacon fat and cigar, turns into blackberries\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 4.5\n",
      "Layered, sweet baked goods, enriched with rye spiciness. If only Bulleit could finally do sth about their constantly chipping corks!\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 4.75\n",
      "\n",
      "Negative SHORT Reviews: \n",
      "Thin delicate , sweetness high high pep, berry, 1.5  Thin berry blackcurrent sweetness, thin, delicate 1.5  Mid sustained thin 1 1.25\n",
      "Predicted Rating - 0.7311\n",
      "Actual Rating - 1.25\n",
      "Ryes just are not for me. Did not enjoy, had a very odd turn at the end that did not please me.  Brooke did not like at all.\n",
      "Predicted Rating - 0.731\n",
      "Actual Rating - 0.75\n",
      "Nicely balanced but perhaps on the bland side.\n",
      "Predicted Rating - 0.7193\n",
      "Actual Rating - 2.5\n"
     ]
    }
   ],
   "source": [
    "### DOES IT PREDICT SHORT REVIEWS WELL?\n",
    "\n",
    "print('\\nPositive SHORT Reviews: ')\n",
    "rand_sample = random.sample(list(range(len(df_short_pos))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_short_pos['rev_notes'][samp_idx]\n",
    "    samp_rating = df_short_pos['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predicted_label_proba, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))\n",
    "    \n",
    "print('\\nNegative SHORT Reviews: ')\n",
    "rand_sample = random.sample(list(range(len(df_short_neg))), 3)\n",
    "for samp_idx in rand_sample:\n",
    "    samp_review = df_short_neg['rev_notes'][samp_idx]\n",
    "    samp_rating = df_short_neg['rev_rating'][samp_idx]\n",
    "    print(samp_review)\n",
    "    predicted_label_index, predicted_label_proba = predict(model_long_revs, samp_review)\n",
    "    print('Predicted Rating - ' + str(round(predicted_label_proba, 4)))\n",
    "    print('Actual Rating - ' + str(samp_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "str_rand = df_ratings_all['rev_notes'].loc[df_ratings_all['rev_char_len'] == 40].iloc[80]\n",
    "str_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_all['rev_notes'][1025]"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
